# è­¦å‘Šã‚’æœ€åˆã«æŠ‘åˆ¶ï¼ˆimportã®å‰ã«å®Ÿè¡Œï¼‰
import warnings
import os
warnings.filterwarnings("ignore")
# torch.classesè­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆStreamlitäº’æ›æ€§å•é¡Œï¼‰
warnings.filterwarnings("ignore", ".*torch._classes.*")
warnings.filterwarnings("ignore", ".*torch.*")
warnings.filterwarnings("ignore", ".*Examining the path of torch.classes.*")
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['PYTHONWARNINGS'] = 'ignore'
# PyTorch classesè­¦å‘Šã‚’å®Œå…¨ã«æŠ‘åˆ¶
os.environ['PYTHONHASHSEED'] = 'ignore'

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchaudio
import librosa
from pathlib import Path
import yaml
import time
import logging
from typing import Tuple, Optional
import tempfile
import threading
from datetime import datetime

# Simple recorderãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
try:
    from simple_recorder import SimpleAudioRecorder
    RECORDER_AVAILABLE = True
    RECORDER_ERROR = None
except ImportError as e:
    # sounddeviceãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    from simple_recorder_fallback import FallbackAudioRecorder as SimpleAudioRecorder
    RECORDER_AVAILABLE = False
    RECORDER_ERROR = str(e)
except Exception as e:
    from simple_recorder_fallback import FallbackAudioRecorder as SimpleAudioRecorder
    RECORDER_AVAILABLE = False
    RECORDER_ERROR = str(e)

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="éŸ³å£°æ¤œå‡ºã‚¢ãƒ—ãƒª - Simple Recording",
    page_icon="ğŸµ",
    layout="wide"
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆreference/config.yamlã«åˆã‚ã›ãŸæ§‹é€ ï¼‰
DEFAULT_CONFIG = {
    'audio': {'sample_rate': 44100},
    'model': {
        'input_length': 44100, 
        'num_classes': 2,
        'cnn_layers': [
            {'filters': 64, 'kernel_size': 3, 'stride': 1, 'padding': 'same'},
            {'filters': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1},
            {'filters': 256, 'kernel_size': 3, 'stride': 2, 'padding': 1}
        ],
        'attention': {
            'hidden_dim': 256,
            'num_heads': 8
        },
        'fully_connected': [
            {'units': 512, 'dropout': 0.3},
            {'units': 256, 'dropout': 0.3}
        ]
    }
}

# Multi-head attention layer for audio feature processing
class AttentionLayer(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int):
        super(AttentionLayer, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        batch_size, seq_len, input_dim = x.size()
        
        # Compute queries, keys, values
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        attended = attended.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        output = self.output(attended)
        return output

# éŸ³å£°ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆ1D-CNN + Attentionæ©Ÿæ§‹ï¼‰
class SoundAnomalyDetector(nn.Module):
    def __init__(self, config: dict):
        super(SoundAnomalyDetector, self).__init__()
        self.config = config
        
        # CNN layers
        cnn_layers = []
        input_channels = 1
        
        for layer_config in config['model']['cnn_layers']:
            # Handle "same" padding by calculating appropriate padding
            padding = layer_config['padding']
            if padding == 'same':
                # For "same" padding in PyTorch, use kernel_size//2
                padding = layer_config['kernel_size'] // 2
            
            cnn_layers.extend([
                nn.Conv1d(
                    input_channels, 
                    layer_config['filters'],
                    kernel_size=layer_config['kernel_size'],
                    stride=layer_config['stride'],
                    padding=padding
                ),
                nn.BatchNorm1d(layer_config['filters']),
                nn.ReLU(),
                nn.MaxPool1d(2)
            ])
            input_channels = layer_config['filters']
        
        self.cnn = nn.Sequential(*cnn_layers)
        
        # Calculate CNN output size
        self.cnn_output_size = self._get_cnn_output_size(config['model']['input_length'])
        
        # Attention layer
        attention_config = config['model']['attention']
        self.attention = AttentionLayer(
            input_channels,
            attention_config['hidden_dim'],
            attention_config['num_heads']
        )
        
        # Fully connected layers
        fc_layers = []
        fc_input_size = attention_config['hidden_dim']
        
        for fc_config in config['model']['fully_connected']:
            fc_layers.extend([
                nn.Linear(fc_input_size, fc_config['units']),
                nn.ReLU(),
                nn.Dropout(fc_config['dropout'])
            ])
            fc_input_size = fc_config['units']
        
        # Output layer
        fc_layers.append(nn.Linear(fc_input_size, config['model']['num_classes']))
        self.classifier = nn.Sequential(*fc_layers)
        
        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
    
    def _get_cnn_output_size(self, input_length: int) -> int:
        # Create dummy input to calculate output size
        x = torch.randn(1, 1, input_length)
        with torch.no_grad():
            x = self.cnn(x)
        return x.size(-1)
    
    def forward(self, x):
        # Input shape: (batch_size, input_length)
        # Add channel dimension: (batch_size, 1, input_length)
        x = x.unsqueeze(1)
        
        # CNN feature extraction
        x = self.cnn(x)  # (batch_size, channels, reduced_length)
        
        # Prepare for attention: (batch_size, seq_len, features)
        x = x.transpose(1, 2)
        
        # Apply attention
        x = self.attention(x)
        
        # Global average pooling
        x = x.transpose(1, 2)  # Back to (batch_size, features, seq_len)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, features)
        
        # Classification
        output = self.classifier(x)
        
        return output

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
def load_config():
    """reference/config.yamlãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨"""
    config_path = Path('reference/config.yaml')
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
            logger.info("âœ… reference/config.yamlã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return yaml_config
        except Exception as e:
            logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        logger.info("reference/config.yamlãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
    
    return DEFAULT_CONFIG

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
def load_model(model_file=None):
    # è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆreference/config.yamlãŒã‚ã‚Œã°å„ªå…ˆä½¿ç”¨ï¼‰
    config = load_config()
    model = SoundAnomalyDetector(config)
    
    if model_file is not None:
        try:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            import io
            model_data = torch.load(io.BytesIO(model_file.read()), map_location='cpu')
            model.load_state_dict(model_data)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.success("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        # referenceãƒ•ã‚©ãƒ«ãƒ€ã®best_model.pthã‚’ãƒã‚§ãƒƒã‚¯
        reference_model_path = Path('reference/best_model.pth')
        default_model_path = Path('models/best_model.pth')
        
        model_loaded = False
        
        # ã¾ãšreferenceãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦ã™
        if reference_model_path.exists():
            try:
                state_dict = torch.load(reference_model_path, map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("reference ãƒ•ã‚©ãƒ«ãƒ€ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.success("âœ… referenceãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                model_loaded = True
            except Exception as e:
                logger.warning(f"referenceãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        # æ¬¡ã«modelsãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦ã™
        if not model_loaded and default_model_path.exists():
            try:
                state_dict = torch.load(default_model_path, map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("models ãƒ•ã‚©ãƒ«ãƒ€ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.info("ğŸ“ modelsãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ä¸­")
                model_loaded = True
            except Exception as e:
                logger.warning(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        if not model_loaded:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™")
            st.info("ğŸ¤– ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            _initialize_baseline_model(model)
    
    model.eval()
    return model

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def _initialize_baseline_model(model):
    try:
        for name, param in model.named_parameters():
            if 'weight' in name:
                if len(param.shape) > 1:
                    torch.nn.init.xavier_uniform_(param)
                else:
                    torch.nn.init.uniform_(param, -0.1, 0.1)
            elif 'bias' in name:
                torch.nn.init.constant_(param, 0)
        logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")

# éŸ³å£°å‰å‡¦ç†
def preprocess_audio(audio_data, sample_rate=44100):
    # æ­£è¦åŒ–
    audio_data = audio_data.astype(np.float32)
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    # 1ç§’é–“ï¼ˆ44100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«èª¿æ•´
    if len(audio_data) > sample_rate:
        audio_data = audio_data[:sample_rate]
    elif len(audio_data) < sample_rate:
        audio_data = np.pad(audio_data, (0, sample_rate - len(audio_data)), mode='constant')
    
    return audio_data

# éŸ³å£°åˆ†æ
def analyze_audio(audio_data, model, sample_rate=44100):
    # 1ç§’æ¯ã«åˆ†å‰²
    segments = []
    predictions = []
    confidences = []
    
    total_seconds = len(audio_data) // sample_rate
    
    for i in range(total_seconds):
        start_idx = i * sample_rate
        end_idx = start_idx + sample_rate
        segment = audio_data[start_idx:end_idx]
        
        # å‰å‡¦ç†
        processed_segment = preprocess_audio(segment, sample_rate)
        segments.append(processed_segment)
        
        # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        with torch.no_grad():
            input_tensor = torch.tensor(processed_segment).unsqueeze(0)
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            
            confidence, prediction = torch.max(probabilities, 1)
            predictions.append(prediction.item())
            confidences.append(confidence.item())
    
    return segments, predictions, confidences

# çµæœãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
def plot_results(audio_data, predictions, sample_rate=44100, confidences=None):
    fig, ax = plt.subplots(figsize=(15, 6))
    
    # æ™‚é–“è»¸
    time_axis = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))
    
    # æ³¢å½¢ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(time_axis, audio_data, color='blue', alpha=0.7, linewidth=0.5)
    
    # OK/NGåŒºé–“ã®èƒŒæ™¯è‰²
    for i, pred in enumerate(predictions):
        start_time = i
        end_time = i + 1
        color = 'lightgreen' if pred == 0 else 'lightcoral'
        label = 'OK' if pred == 0 else 'NG'
        
        # ä¿¡é ¼åº¦ã«å¿œã˜ã¦ã‚¢ãƒ«ãƒ•ã‚¡å€¤ã‚’èª¿æ•´
        if confidences and i < len(confidences):
            alpha = 0.2 + (confidences[i] * 0.4)  # 0.2-0.6ã®ç¯„å›²
            confidence_text = f" ({confidences[i]:.2f})"
        else:
            alpha = 0.3
            confidence_text = ""
        
        ax.axvspan(start_time, end_time, alpha=alpha, color=color)
        
        # ä¸­å¤®ã«ãƒ©ãƒ™ãƒ«è¡¨ç¤ºï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
        mid_time = start_time + 0.5
        ax.text(mid_time, max(audio_data) * 0.8, f"{label}{confidence_text}", 
                ha='center', va='center', fontsize=10, fontweight='bold')
        
        # ä¿¡é ¼åº¦ãƒãƒ¼ã‚’ä¸‹éƒ¨ã«è¡¨ç¤º
        if confidences and i < len(confidences):
            bar_height = min(audio_data) * 0.1 * confidences[i]
            ax.axvspan(start_time, end_time, ymin=0, ymax=0.05, 
                      alpha=0.8, color='darkblue')
    
    ax.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax.set_ylabel('æŒ¯å¹…', fontsize=12)
    ax.set_title('éŸ³å£°æ³¢å½¢ã¨ç•°å¸¸æ¤œçŸ¥çµæœï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # å‡¡ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', alpha=0.5, label='OK (æ­£å¸¸)'),
        Patch(facecolor='lightcoral', alpha=0.5, label='NG (ç•°å¸¸)'),
        Patch(facecolor='darkblue', alpha=0.8, label='ä¿¡é ¼åº¦ãƒãƒ¼')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    return fig

# WAVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•°
def process_wav_file(file_path: str, model, sample_rate: int = 44100):
    """
    WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†
    
    Args:
        file_path: WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        model: éŸ³å£°æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°
        
    Returns:
        tuple: (éŸ³å£°ãƒ‡ãƒ¼ã‚¿, äºˆæ¸¬çµæœ, ä¿¡é ¼åº¦)
    """
    try:
        # WAVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        audio_data, sr = SimpleAudioRecorder.load_wav_file(file_path)
        
        if len(audio_data) == 0:
            return None, None, None
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if sr != sample_rate:
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=sample_rate)
        
        # éŸ³å£°åˆ†æ
        segments, predictions, confidences = analyze_audio(audio_data, model, sample_rate)
        
        return audio_data, predictions, confidences
        
    except Exception as e:
        logger.error(f"WAVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None, None

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    st.title("ğŸµ éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ—ãƒª - Simple Recording")
    st.markdown("**ã‚·ãƒ³ãƒ—ãƒ«ãªéŒ²éŸ³æ©Ÿèƒ½ã§WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€1ç§’æ¯ã«OK/NGã‚’åˆ¤å®šã—ã¾ã™**")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")
    
    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹æƒ…å ±è¡¨ç¤º
    with st.sidebar.expander("ğŸ¤ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹æƒ…å ±"):
        try:
            devices = SimpleAudioRecorder.get_available_devices()
            if devices:
                st.success(f"âœ… {len(devices)}å€‹ã®ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½")
                for device in devices:
                    st.write(f"â€¢ {device['name']} ({device['channels']}ch)")
            else:
                st.warning("âš ï¸ å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                st.info("""**ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ç†ç”±:**
                â€¢ ãƒã‚¤ã‚¯ãŒæ¥ç¶šã•ã‚Œã¦ã„ãªã„
                â€¢ æ¨©é™è¨­å®šãŒå¿…è¦
                â€¢ ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®å•é¡Œ
                â€¢ ä»–ã®ã‚¢ãƒ—ãƒªãŒãƒã‚¤ã‚¯ã‚’å æœ‰""")
                st.markdown("**è§£æ±ºæ–¹æ³•:**")
                st.code("python install_dependencies.py")
        except Exception as e:
            st.error(f"âŒ ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Audio device query error: {e}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.sidebar.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    uploaded_model = st.sidebar.file_uploader(
        "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (.pth) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['pth'],
        help="è¨“ç·´æ¸ˆã¿PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    sample_rate = st.sidebar.selectbox("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°", [22050, 44100], index=1)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if 'model' not in st.session_state or uploaded_model:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            st.session_state.model = load_model(uploaded_model)
    
    # éŒ²éŸ³è¨­å®š
    st.sidebar.subheader("ğŸ“¹ éŒ²éŸ³è¨­å®š")
    recording_duration = st.sidebar.slider("éŒ²éŸ³æ™‚é–“ (ç§’)", 1, 30, 5)
    
    # éŸ³å£°éŒ²éŸ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.header("ğŸ¤ éŸ³å£°éŒ²éŸ³ (WAVä¿å­˜)")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("**Simple Recorderã‚’ä½¿ç”¨ã—ãŸWAVéŒ²éŸ³æ©Ÿèƒ½**")
        
        # éŒ²éŸ³æ©Ÿèƒ½ã®åˆ©ç”¨å¯å¦ã‚’è¡¨ç¤º
        if not RECORDER_AVAILABLE:
            st.warning("âš ï¸ éŒ²éŸ³æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            st.info(f"**ã‚¨ãƒ©ãƒ¼è©³ç´°**: {RECORDER_ERROR}")
            
            # ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã®è§£æ±ºæ–¹æ³•ã‚’è¡¨ç¤º
            with st.expander("ğŸ”§ éŒ²éŸ³æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹æ–¹æ³•", expanded=True):
                st.markdown("""
                éŒ²éŸ³æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
                
                **1. ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
                ```bash
                # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
                python install_dependencies.py
                ```
                
                **2. æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆä¸Šè¨˜ãŒå¤±æ•—ã—ãŸå ´åˆï¼‰**
                
                **Google Colabç’°å¢ƒã®å ´åˆ:**
                ```bash
                !apt-get update -qq
                !apt-get install -y portaudio19-dev python3-pyaudio alsa-utils
                !pip install sounddevice>=0.4.0
                ```
                
                **Linuxç’°å¢ƒã®å ´åˆ:**
                ```bash
                sudo apt-get update
                sudo apt-get install -y portaudio19-dev python3-pyaudio alsa-utils
                pip install sounddevice>=0.4.0
                ```
                
                **3. ä»£æ›¿æ‰‹æ®µ**
                ç¾åœ¨ã¯**WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½**ã‚’ã”åˆ©ç”¨ãã ã•ã„ï¼ˆä¸‹è¨˜å‚ç…§ï¼‰
                """)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’å¼·èª¿è¡¨ç¤º
            st.markdown("---")
            st.markdown("### ğŸ“‚ ä»£æ›¿æ‰‹æ®µ: WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
            if st.button("ğŸ“¤ WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã«ç§»å‹•", use_container_width=True):
                st.rerun()
            st.markdown("---")
        else:
            st.success("âœ… éŒ²éŸ³æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        
        # éŒ²éŸ³åˆ¶å¾¡
        if 'recorder' not in st.session_state:
            st.session_state.recorder = SimpleAudioRecorder(sample_rate=sample_rate)
        
        recorder = st.session_state.recorder
        
        # éŒ²éŸ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if 'is_recording' not in st.session_state:
            st.session_state.is_recording = False
        if 'last_recording_data' not in st.session_state:
            st.session_state.last_recording_data = None
        if 'last_wav_file' not in st.session_state:
            st.session_state.last_wav_file = None
        
        # éŒ²éŸ³ãƒœã‚¿ãƒ³
        col_rec1, col_rec2, col_rec3 = st.columns(3)
        
        with col_rec1:
            if st.button("ğŸ™ï¸ éŒ²éŸ³é–‹å§‹", type="primary", disabled=st.session_state.is_recording):
                try:
                    # éŒ²éŸ³ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    wav_filename = f"recording_{timestamp}.wav"
                    wav_path = Path("recordings") / wav_filename
                    
                    # recordingsãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
                    Path("recordings").mkdir(exist_ok=True)
                    
                    st.session_state.is_recording = True
                    st.session_state.current_wav_file = str(wav_path)
                    
                    # é€²æ—è¡¨ç¤ºç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # éŒ²éŸ³å®Ÿè¡Œï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
                    def record_audio():
                        # Streamlitã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã€ç›´æ¥çŠ¶æ…‹æ›´æ–°ã¯é¿ã‘ã‚‹
                        progress_data = {'current': 0, 'total': recording_duration}
                        
                        def progress_callback(current_time, total_time):
                            progress_data['current'] = current_time
                            # UIæ›´æ–°ã¯ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã«å§”è­²ï¼ˆãƒ­ã‚°ã®ã¿è¨˜éŒ²ï¼‰
                            logger.info(f"éŒ²éŸ³é€²æ—: {current_time:.1f}/{total_time:.1f}ç§’")
                        
                        success = recorder.record_and_save(
                            duration=recording_duration,
                            file_path=str(wav_path),
                            progress_callback=progress_callback
                        )
                        
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®æ›´æ–°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
                        st.session_state.is_recording = False
                        st.session_state.recording_result = {
                            'success': success,
                            'wav_path': str(wav_path),
                            'wav_filename': wav_filename
                        }
                    
                    # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
                    recording_thread = threading.Thread(target=record_audio)
                    recording_thread.daemon = True
                    recording_thread.start()
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰
                    while st.session_state.is_recording and recording_thread.is_alive():
                        progress_bar.progress(min(time.time() - time.time(), 1.0))
                        status_text.text(f"éŒ²éŸ³ä¸­...")
                        time.sleep(0.1)
                    
                    # çµæœå‡¦ç†
                    if 'recording_result' in st.session_state:
                        result = st.session_state.recording_result
                        if result['success']:
                            st.session_state.last_wav_file = result['wav_path']
                            st.success(f"âœ… éŒ²éŸ³å®Œäº†ï¼ {result['wav_filename']} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
                            
                            # éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                            audio_data, sr = SimpleAudioRecorder.load_wav_file(result['wav_path'])
                            st.session_state.last_recording_data = audio_data
                        else:
                            st.error("âŒ éŒ²éŸ³ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        
                        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        del st.session_state.recording_result
                    
                    progress_bar.empty()
                    status_text.empty()
                    
                except Exception as e:
                    st.error(f"éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
                    st.session_state.is_recording = False
        
        with col_rec2:
            if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", type="secondary", disabled=not st.session_state.is_recording):
                if recorder.is_recording:
                    audio_data = recorder.stop_recording()
                    if len(audio_data) > 0:
                        # WAVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        wav_filename = f"recording_{timestamp}_manual.wav"
                        wav_path = Path("recordings") / wav_filename
                        Path("recordings").mkdir(exist_ok=True)
                        
                        if recorder.save_to_wav(audio_data, str(wav_path)):
                            st.session_state.last_wav_file = str(wav_path)
                            st.session_state.last_recording_data = audio_data
                            st.success(f"âœ… éŒ²éŸ³åœæ­¢ãƒ»ä¿å­˜å®Œäº†ï¼ {wav_filename}")
                        else:
                            st.error("âŒ WAVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    else:
                        st.warning("éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                
                st.session_state.is_recording = False
        
        with col_rec3:
            if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
                # éŒ²éŸ³åœæ­¢
                if recorder.is_recording:
                    recorder.stop_recording()
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢
                for key in ['last_recording_data', 'last_wav_file', 'is_recording', 
                           'audio_data', 'predictions', 'confidences']:
                    if key in st.session_state:
                        del st.session_state[key]
                
                st.success("ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
                st.rerun()
        
        # éŒ²éŸ³çŠ¶æ…‹è¡¨ç¤º
        if st.session_state.is_recording:
            st.warning("ğŸ”´ éŒ²éŸ³ä¸­...")
        elif st.session_state.last_wav_file:
            st.info(f"ğŸ“ æœ€æ–°ã®éŒ²éŸ³: {Path(st.session_state.last_wav_file).name}")
        
        # WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æãƒœã‚¿ãƒ³
        if st.session_state.last_wav_file and not st.session_state.is_recording:
            if st.button("ğŸ” WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ", type="primary"):
                with st.spinner("WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­..."):
                    audio_data, predictions, confidences = process_wav_file(
                        st.session_state.last_wav_file, 
                        st.session_state.model, 
                        sample_rate
                    )
                    
                    if audio_data is not None:
                        # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                        st.session_state.audio_data = audio_data
                        st.session_state.predictions = predictions
                        st.session_state.confidences = confidences
                        st.session_state.sample_rate = sample_rate
                        st.success("ğŸ¯ WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Œäº†ï¼")
                    else:
                        st.error("WAVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    with col2:
        st.markdown("### ğŸ“Š éŒ²éŸ³çŠ¶æ³")
        
        if st.session_state.last_recording_data is not None:
            duration = len(st.session_state.last_recording_data) / sample_rate
            st.metric("éŒ²éŸ³æ™‚é–“", f"{duration:.1f}ç§’")
            st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", f"{len(st.session_state.last_recording_data):,} ã‚µãƒ³ãƒ—ãƒ«")
            st.metric("ãƒ•ã‚¡ã‚¤ãƒ«", Path(st.session_state.last_wav_file).name if st.session_state.last_wav_file else "ãªã—")
        else:
            st.metric("éŒ²éŸ³æ™‚é–“", "0.0ç§’")
            st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", "0 ã‚µãƒ³ãƒ—ãƒ«")
            st.metric("ãƒ•ã‚¡ã‚¤ãƒ«", "ãªã—")
    
    # ä¿å­˜ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    st.header("ğŸ“ ä¿å­˜ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«")
    recordings_path = Path("recordings")
    if recordings_path.exists():
        wav_files = list(recordings_path.glob("*.wav"))
        if wav_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
            selected_file = st.selectbox(
                "WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                options=[f.name for f in sorted(wav_files, reverse=True)],
                help="åˆ†æã—ãŸã„WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )
            
            if selected_file:
                selected_path = recordings_path / selected_file
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("ğŸ” é¸æŠãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"):
                        with st.spinner(f"{selected_file} ã‚’åˆ†æä¸­..."):
                            audio_data, predictions, confidences = process_wav_file(
                                str(selected_path), 
                                st.session_state.model, 
                                sample_rate
                            )
                            
                            if audio_data is not None:
                                st.session_state.audio_data = audio_data
                                st.session_state.predictions = predictions
                                st.session_state.confidences = confidences
                                st.session_state.sample_rate = sample_rate
                                st.success(f"âœ… {selected_file} ã®åˆ†æå®Œäº†ï¼")
                            else:
                                st.error(f"âŒ {selected_file} ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                with col2:
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    if selected_path.exists():
                        with open(selected_path, "rb") as f:
                            st.download_button(
                                label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=f.read(),
                                file_name=selected_file,
                                mime="audio/wav"
                            )
                
                with col3:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    if st.button("ğŸ—‘ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤", type="secondary"):
                        if selected_path.exists():
                            selected_path.unlink()
                            st.success(f"âœ… {selected_file} ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                            st.rerun()
        else:
            st.info("ä¿å­˜ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
    else:
        st.info("recordingsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    if not RECORDER_AVAILABLE:
        st.header("ğŸ“¤ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ï¼‰")
        st.info("ğŸ¤ éŒ²éŸ³æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’ãƒ¡ã‚¤ãƒ³ã¨ã—ã¦ã”åˆ©ç”¨ãã ã•ã„")
    else:
        st.header("ğŸ“¤ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    uploaded_audio = st.file_uploader(
        "å¤–éƒ¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['wav', 'mp3', 'flac', 'm4a'],
        help="å¤–éƒ¨ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã§ãã¾ã™ã€‚éŒ²éŸ³æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ããªã„ç’°å¢ƒã§ã¯ã€ã“ã®æ©Ÿèƒ½ã‚’ãƒ¡ã‚¤ãƒ³ã¨ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚"
    )
    
    if uploaded_audio:
        with st.spinner("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."):
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{uploaded_audio.name.split(".")[-1]}') as tmp_file:
                    tmp_file.write(uploaded_audio.read())
                    tmp_path = tmp_file.name
                
                # librosã§éŸ³å£°èª­ã¿è¾¼ã¿
                audio_data, sr = librosa.load(tmp_path, sr=sample_rate, mono=True)
                
                if len(audio_data) > sample_rate:  # æœ€ä½1ç§’å¿…è¦
                    st.success("âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
                    
                    # éŸ³å£°åˆ†æ
                    with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                        segments, predictions, confidences = analyze_audio(
                            audio_data, st.session_state.model, sample_rate
                        )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.audio_data = audio_data
                    st.session_state.predictions = predictions
                    st.session_state.confidences = confidences
                    st.session_state.sample_rate = sample_rate
                else:
                    st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒçŸ­ã™ãã¾ã™ã€‚æœ€ä½1ç§’ä»¥ä¸Šã®éŸ³å£°ãŒå¿…è¦ã§ã™ã€‚")
                    
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                Path(tmp_path).unlink()
                
            except Exception as e:
                st.error(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœè¡¨ç¤º
    if 'audio_data' in st.session_state and 'predictions' in st.session_state:
        st.header("ğŸ“Š åˆ†æçµæœ")
        
        # çµ±è¨ˆæƒ…å ±
        predictions = st.session_state.predictions
        confidences = st.session_state.confidences
        ok_count = predictions.count(0)
        ng_count = predictions.count(1)
        
        # ä¿¡é ¼åº¦çµ±è¨ˆ
        ok_confidences = [conf for pred, conf in zip(predictions, confidences) if pred == 0]
        ng_confidences = [conf for pred, conf in zip(predictions, confidences) if pred == 1]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·æ™‚é–“", f"{len(predictions)}ç§’")
        with col2:
            st.metric("OKï¼ˆæ­£å¸¸ï¼‰", f"{ok_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", 
                     delta=f"ä¿¡é ¼åº¦: {sum(ok_confidences)/len(ok_confidences):.2f}" if ok_confidences else "ä¿¡é ¼åº¦: N/A")
        with col3:
            st.metric("NGï¼ˆç•°å¸¸ï¼‰", f"{ng_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ",
                     delta=f"ä¿¡é ¼åº¦: {sum(ng_confidences)/len(ng_confidences):.2f}" if ng_confidences else "ä¿¡é ¼åº¦: N/A")
        with col4:
            st.metric("å¹³å‡ä¿¡é ¼åº¦", f"{avg_confidence:.3f}")
        
        # ä¿¡é ¼åº¦ã®è‰²åˆ†ã‘è¡¨ç¤º
        st.subheader("ğŸ¯ ä¿¡é ¼åº¦ã‚µãƒãƒªãƒ¼")
        
        confidence_cols = st.columns(len(predictions))
        for i, (pred, conf) in enumerate(zip(predictions, confidences)):
            with confidence_cols[i]:
                status = "âœ… OK" if pred == 0 else "âŒ NG"
                color = "green" if pred == 0 else "red"
                st.markdown(f"**{i+1}ç§’ç›®**")
                st.markdown(f"<div style='color: {color}; font-weight: bold;'>{status}</div>", unsafe_allow_html=True)
                st.progress(conf)
                st.caption(f"ä¿¡é ¼åº¦: {conf:.3f}")
        
        # æ³¢å½¢ã‚°ãƒ©ãƒ•ï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
        fig = plot_results(st.session_state.audio_data, predictions, 
                          st.session_state.sample_rate, confidences)
        st.pyplot(fig)
        
        # è©³ç´°çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("ğŸ“‹ è©³ç´°çµæœãƒ†ãƒ¼ãƒ–ãƒ«")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        import pandas as pd
        
        results_data = []
        for i, (pred, conf) in enumerate(zip(predictions, confidences)):
            status_emoji = "âœ…" if pred == 0 else "âŒ"
            status_text = "OK (æ­£å¸¸)" if pred == 0 else "NG (ç•°å¸¸)"
            confidence_level = "é«˜" if conf > 0.8 else "ä¸­" if conf > 0.5 else "ä½"
            
            results_data.append({
                "æ™‚åˆ»": f"{i+1}ç§’ç›®",
                "åˆ¤å®š": f"{status_emoji} {status_text}",
                "ä¿¡é ¼åº¦": f"{conf:.3f}",
                "ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«": confidence_level,
                "ç¢ºä¿¡åº¦": f"{conf*100:.1f}%"
            })
        
        results_df = pd.DataFrame(results_data)
        st.dataframe(results_df, use_container_width=True)
    
    # èª¬æ˜
    st.markdown("---")
    st.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    #### ğŸ¤ Simple Recordingæ–¹å¼:
    1. **éŒ²éŸ³æ™‚é–“è¨­å®š**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§éŒ²éŸ³æ™‚é–“ã‚’é¸æŠ
    2. **éŒ²éŸ³é–‹å§‹**: ã€ŒğŸ™ï¸ éŒ²éŸ³é–‹å§‹ã€ãƒœã‚¿ãƒ³ã§WAVéŒ²éŸ³é–‹å§‹
    3. **è‡ªå‹•åœæ­¢**: è¨­å®šæ™‚é–“ã§è‡ªå‹•åœæ­¢ãƒ»WAVä¿å­˜
    4. **åˆ†æå®Ÿè¡Œ**: ã€ŒğŸ” WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã€ã§è§£æé–‹å§‹
    
    #### ğŸ“ WAVãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†:
    - éŒ²éŸ³ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«ã¯`recordings/`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
    - ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‹ã‚‰éå»ã®éŒ²éŸ³ã‚’é¸æŠãƒ»åˆ†æå¯èƒ½
    - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰Šé™¤æ©Ÿèƒ½ä»˜ã
    
    #### ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:
    - å¤–éƒ¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAV, MP3ç­‰ï¼‰ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»åˆ†æ
    
    **åˆ¤å®šã«ã¤ã„ã¦:**
    - ğŸŸ¢ **OKï¼ˆæ­£å¸¸ï¼‰**: æ­£å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    - ğŸ”´ **NGï¼ˆç•°å¸¸ï¼‰**: ç•°å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    
    **æ”¹å–„ç‚¹:**
    - WebRTCã«ã‚ˆã‚‹è¤‡é›‘ãªéŒ²éŸ³å‡¦ç†ã‚’æ’é™¤
    - sounddeviceã«ã‚ˆã‚‹å®‰å®šã—ãŸéŒ²éŸ³
    - WAVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ç›´æ¥ä¿å­˜
    - ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†æ©Ÿèƒ½ã®è¿½åŠ 
    """)

if __name__ == "__main__":
    main()