import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchaudio
import librosa
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
from pathlib import Path
import yaml
import time
import logging
from typing import Tuple, Optional
import warnings
import io
import tempfile

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore")

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="éŸ³å£°æ¤œå‡ºã‚¢ãƒ—ãƒª",
    page_icon="ğŸµ",
    layout="wide"
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
DEFAULT_CONFIG = {
    'audio': {'sample_rate': 44100},
    'model': {'input_length': 44100, 'num_classes': 2}
}

# éŸ³å£°ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆå‚è€ƒãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç°¡ç•¥åŒ–ï¼‰
class SimpleAnomalyDetector(nn.Module):
    def __init__(self, input_length=44100, num_classes=2):
        super().__init__()
        
        # 1D CNN layers
        self.conv1 = nn.Conv1d(1, 32, kernel_size=256, stride=4)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=128, stride=2)
        self.conv3 = nn.Conv1d(64, 128, kernel_size=64, stride=2)
        
        self.pool = nn.MaxPool1d(4)
        self.dropout = nn.Dropout(0.5)
        
        # FC layers
        self.fc1 = nn.Linear(self._get_conv_output_size(input_length), 256)
        self.fc2 = nn.Linear(256, num_classes)
        
    def _get_conv_output_size(self, input_length):
        # ç•³ã¿è¾¼ã¿å±¤ã®å‡ºåŠ›ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        x = torch.randn(1, 1, input_length)
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        return x.view(1, -1).size(1)
    
    def forward(self, x):
        # å…¥åŠ›ã¯ (batch_size, input_length)
        x = x.unsqueeze(1)  # (batch_size, 1, input_length)
        
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        x = self.fc2(x)
        
        return x

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
def load_model(model_file=None):
    model = SimpleAnomalyDetector(
        input_length=DEFAULT_CONFIG['model']['input_length'],
        num_classes=DEFAULT_CONFIG['model']['num_classes']
    )
    
    if model_file is not None:
        try:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model_data = torch.load(io.BytesIO(model_file.read()), map_location='cpu')
            model.load_state_dict(model_data)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.success("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        default_model_path = Path('models/best_model.pth')
        if default_model_path.exists():
            try:
                model.load_state_dict(torch.load(default_model_path, map_location='cpu'))
                logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.info("ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ä¸­")
            except Exception as e:
                logger.warning(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
        else:
            st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    model.eval()
    return model

# éŸ³å£°å‰å‡¦ç†
def preprocess_audio(audio_data, sample_rate=44100):
    # æ­£è¦åŒ–
    audio_data = audio_data.astype(np.float32)
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    # 1ç§’é–“ï¼ˆ44100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«èª¿æ•´
    if len(audio_data) > sample_rate:
        audio_data = audio_data[:sample_rate]
    elif len(audio_data) < sample_rate:
        audio_data = np.pad(audio_data, (0, sample_rate - len(audio_data)), mode='constant')
    
    return audio_data

# éŸ³å£°åˆ†æ
def analyze_audio(audio_data, model, sample_rate=44100):
    # 1ç§’æ¯ã«åˆ†å‰²
    segments = []
    predictions = []
    confidences = []
    
    total_seconds = len(audio_data) // sample_rate
    
    for i in range(total_seconds):
        start_idx = i * sample_rate
        end_idx = start_idx + sample_rate
        segment = audio_data[start_idx:end_idx]
        
        # å‰å‡¦ç†
        processed_segment = preprocess_audio(segment, sample_rate)
        segments.append(processed_segment)
        
        # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        with torch.no_grad():
            input_tensor = torch.tensor(processed_segment).unsqueeze(0)
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            
            confidence, prediction = torch.max(probabilities, 1)
            predictions.append(prediction.item())
            confidences.append(confidence.item())
    
    return segments, predictions, confidences

# çµæœãƒ—ãƒ­ãƒƒãƒˆ
def plot_results(audio_data, predictions, sample_rate=44100):
    fig, ax = plt.subplots(figsize=(15, 6))
    
    # æ™‚é–“è»¸
    time_axis = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))
    
    # æ³¢å½¢ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(time_axis, audio_data, color='blue', alpha=0.7, linewidth=0.5)
    
    # OK/NGåŒºé–“ã®èƒŒæ™¯è‰²
    for i, pred in enumerate(predictions):
        start_time = i
        end_time = i + 1
        color = 'lightgreen' if pred == 0 else 'lightcoral'
        label = 'OK' if pred == 0 else 'NG'
        
        ax.axvspan(start_time, end_time, alpha=0.3, color=color)
        
        # ä¸­å¤®ã«ãƒ©ãƒ™ãƒ«è¡¨ç¤º
        mid_time = start_time + 0.5
        ax.text(mid_time, max(audio_data) * 0.8, label, 
                ha='center', va='center', fontsize=12, fontweight='bold')
    
    ax.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax.set_ylabel('æŒ¯å¹…', fontsize=12)
    ax.set_title('éŸ³å£°æ³¢å½¢ã¨ç•°å¸¸æ¤œçŸ¥çµæœ', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # å‡¡ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', alpha=0.5, label='OK (æ­£å¸¸)'),
        Patch(facecolor='lightcoral', alpha=0.5, label='NG (ç•°å¸¸)')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    return fig

# WebRTCéŸ³å£°éŒ²éŸ³ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
class AudioProcessor:
    def __init__(self):
        self.audio_frames = []
        self.recording = False
        
    def recv(self, frame):
        if self.recording:
            sound = frame.to_ndarray()
            self.audio_frames.append(sound)
        return frame

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    st.title("ğŸµ éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ—ãƒª")
    st.markdown("**ãƒã‚¤ã‚¯ã§éŒ²éŸ³ã—ã€1ç§’æ¯ã«OK/NGã‚’åˆ¤å®šã—ã¦æ³¢å½¢ä¸Šã«è¡¨ç¤ºã—ã¾ã™**")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.sidebar.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    uploaded_model = st.sidebar.file_uploader(
        "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (.pth) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['pth'],
        help="è¨“ç·´æ¸ˆã¿PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    sample_rate = st.sidebar.selectbox("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°", [22050, 44100], index=1)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if 'model' not in st.session_state or uploaded_model:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            st.session_state.model = load_model(uploaded_model)
    
    # WebRTCè¨­å®š
    RTC_CONFIGURATION = RTCConfiguration({
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    })
    
    # éŸ³å£°éŒ²éŸ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.header("ğŸ¤ éŸ³å£°éŒ²éŸ³")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("**ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŒ²éŸ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„**")
        
        # WebRTCéŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
        audio_processor = AudioProcessor()
        
        webrtc_ctx = webrtc_streamer(
            key="audio-recorder",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={
                "video": False,
                "audio": {
                    "sampleRate": sample_rate,
                    "channelCount": 1,
                    "echoCancellation": False,
                    "noiseSuppression": False,
                    "autoGainControl": False,
                }
            },
            audio_html_attrs={"autoPlay": True, "controls": False, "muted": False},
        )
        
        if webrtc_ctx.audio_receiver:
            # éŒ²éŸ³çŠ¶æ…‹ã®ç®¡ç†
            if st.button("ğŸ™ï¸ éŒ²éŸ³é–‹å§‹", type="primary"):
                st.session_state.recording = True
                st.session_state.audio_buffer = []
                
            if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", type="secondary"):
                st.session_state.recording = False
                
            # éŒ²éŸ³ä¸­ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            if webrtc_ctx.audio_receiver and st.session_state.get('recording', False):
                try:
                    audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)
                    for audio_frame in audio_frames:
                        sound = audio_frame.to_ndarray()
                        if 'audio_buffer' not in st.session_state:
                            st.session_state.audio_buffer = []
                        st.session_state.audio_buffer.append(sound.flatten())
                        
                except Exception as e:
                    logger.warning(f"éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        if st.button("ğŸ” éŸ³å£°åˆ†æ", disabled=not st.session_state.get('audio_buffer')):
            if st.session_state.get('audio_buffer'):
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                audio_data = np.concatenate(st.session_state.audio_buffer)
                
                if len(audio_data) > sample_rate:  # æœ€ä½1ç§’å¿…è¦
                    st.success("âœ… éŒ²éŸ³å®Œäº†ï¼")
                    
                    # éŸ³å£°åˆ†æ
                    with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                        segments, predictions, confidences = analyze_audio(
                            audio_data, st.session_state.model, sample_rate
                        )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.audio_data = audio_data
                    st.session_state.predictions = predictions
                    st.session_state.confidences = confidences
                    st.session_state.sample_rate = sample_rate
                else:
                    st.error("éŒ²éŸ³æ™‚é–“ãŒçŸ­ã™ãã¾ã™ã€‚æœ€ä½1ç§’ä»¥ä¸ŠéŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.error("éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
    
    with col2:
        if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢
            for key in ['audio_data', 'predictions', 'confidences', 'audio_buffer', 'recording']:
                if key in st.session_state:
                    del st.session_state[key]
            st.success("ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            st.rerun()
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ï¼ˆä»£æ›¿æ‰‹æ®µï¼‰
    st.header("ğŸ“ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»£æ›¿æ‰‹æ®µï¼‰")
    uploaded_audio = st.file_uploader(
        "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['wav', 'mp3', 'flac', 'm4a'],
        help="éŒ²éŸ³ã®ä»£ã‚ã‚Šã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™"
    )
    
    if uploaded_audio:
        with st.spinner("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."):
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{uploaded_audio.name.split(".")[-1]}') as tmp_file:
                    tmp_file.write(uploaded_audio.read())
                    tmp_path = tmp_file.name
                
                # librosã§éŸ³å£°èª­ã¿è¾¼ã¿
                audio_data, sr = librosa.load(tmp_path, sr=sample_rate, mono=True)
                
                if len(audio_data) > sample_rate:  # æœ€ä½1ç§’å¿…è¦
                    st.success("âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
                    
                    # éŸ³å£°åˆ†æ
                    with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                        segments, predictions, confidences = analyze_audio(
                            audio_data, st.session_state.model, sample_rate
                        )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.audio_data = audio_data
                    st.session_state.predictions = predictions
                    st.session_state.confidences = confidences
                    st.session_state.sample_rate = sample_rate
                else:
                    st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒçŸ­ã™ãã¾ã™ã€‚æœ€ä½1ç§’ä»¥ä¸Šã®éŸ³å£°ãŒå¿…è¦ã§ã™ã€‚")
                    
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                Path(tmp_path).unlink()
                
            except Exception as e:
                st.error(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœè¡¨ç¤º
    if 'audio_data' in st.session_state and 'predictions' in st.session_state:
        st.header("ğŸ“Š çµæœ")
        
        # çµ±è¨ˆæƒ…å ±
        predictions = st.session_state.predictions
        ok_count = predictions.count(0)
        ng_count = predictions.count(1)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç·æ™‚é–“", f"{len(predictions)}ç§’")
        with col2:
            st.metric("OKï¼ˆæ­£å¸¸ï¼‰", f"{ok_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", delta=None)
        with col3:
            st.metric("NGï¼ˆç•°å¸¸ï¼‰", f"{ng_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", delta=None)
        
        # æ³¢å½¢ã‚°ãƒ©ãƒ•
        fig = plot_results(st.session_state.audio_data, predictions, st.session_state.sample_rate)
        st.pyplot(fig)
        
        # è©³ç´°çµæœ
        with st.expander("ğŸ“‹ è©³ç´°çµæœ"):
            for i, (pred, conf) in enumerate(zip(predictions, st.session_state.confidences)):
                status = "âœ… OK" if pred == 0 else "âŒ NG"
                st.write(f"{i+1}ç§’ç›®: {status} (ä¿¡é ¼åº¦: {conf:.3f})")
    
    # èª¬æ˜
    st.markdown("---")
    st.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    #### ğŸ¤ éŸ³å£°éŒ²éŸ³æ–¹å¼:
    1. **ãƒã‚¤ã‚¯è¨±å¯**: ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ã‚¯ä½¿ç”¨è¨±å¯ã‚’ä¸ãˆã‚‹
    2. **éŒ²éŸ³é–‹å§‹**: ã€ŒğŸ™ï¸ éŒ²éŸ³é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™
    3. **éŒ²éŸ³åœæ­¢**: ã€Œâ¹ï¸ éŒ²éŸ³åœæ­¢ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™
    4. **åˆ†æå®Ÿè¡Œ**: ã€ŒğŸ” éŸ³å£°åˆ†æã€ãƒœã‚¿ãƒ³ã§è§£æé–‹å§‹
    
    #### ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹å¼:
    1. **ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ**: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAV, MP3ç­‰ï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    2. **è‡ªå‹•åˆ†æ**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«è‡ªå‹•ã§è§£æé–‹å§‹
    
    #### ğŸ¤– ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«:
    - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨“ç·´æ¸ˆã¿.pthãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½
    - ãƒ¢ãƒ‡ãƒ«ãªã—ã®å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    
    **åˆ¤å®šã«ã¤ã„ã¦:**
    - ğŸŸ¢ **OKï¼ˆæ­£å¸¸ï¼‰**: æ­£å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    - ğŸ”´ **NGï¼ˆç•°å¸¸ï¼‰**: ç•°å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    """)

if __name__ == "__main__":
    main()