import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchaudio
import librosa
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
from pathlib import Path
import yaml
import time
import logging
from typing import Tuple, Optional
import warnings
import io
import tempfile

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore")
# torch.classesè­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆStreamlitäº’æ›æ€§å•é¡Œï¼‰
warnings.filterwarnings("ignore", ".*torch._classes.*")

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="éŸ³å£°æ¤œå‡ºã‚¢ãƒ—ãƒª",
    page_icon="ğŸµ",
    layout="wide"
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
DEFAULT_CONFIG = {
    'audio': {'sample_rate': 44100},
    'model': {
        'input_length': 44100, 
        'num_classes': 2,
        'cnn_layers': [
            {'filters': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},
            {'filters': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1},
            {'filters': 256, 'kernel_size': 3, 'stride': 2, 'padding': 1}
        ],
        'attention': {
            'hidden_dim': 256,
            'num_heads': 8
        },
        'fully_connected': [
            {'units': 512, 'dropout': 0.3},
            {'units': 256, 'dropout': 0.3}
        ]
    }
}

# Multi-head attention layer for audio feature processing
class AttentionLayer(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int):
        super(AttentionLayer, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        batch_size, seq_len, input_dim = x.size()
        
        # Compute queries, keys, values
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        attended = attended.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        output = self.output(attended)
        return output

# éŸ³å£°ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆ1D-CNN + Attentionæ©Ÿæ§‹ï¼‰
class SoundAnomalyDetector(nn.Module):
    def __init__(self, config: dict):
        super(SoundAnomalyDetector, self).__init__()
        self.config = config
        
        # CNN layers
        cnn_layers = []
        input_channels = 1
        
        for layer_config in config['model']['cnn_layers']:
            cnn_layers.extend([
                nn.Conv1d(
                    input_channels, 
                    layer_config['filters'],
                    kernel_size=layer_config['kernel_size'],
                    stride=layer_config['stride'],
                    padding=layer_config['padding']
                ),
                nn.BatchNorm1d(layer_config['filters']),
                nn.ReLU(),
                nn.MaxPool1d(2)
            ])
            input_channels = layer_config['filters']
        
        self.cnn = nn.Sequential(*cnn_layers)
        
        # Calculate CNN output size
        self.cnn_output_size = self._get_cnn_output_size(config['model']['input_length'])
        
        # Attention layer
        attention_config = config['model']['attention']
        self.attention = AttentionLayer(
            input_channels,
            attention_config['hidden_dim'],
            attention_config['num_heads']
        )
        
        # Fully connected layers
        fc_layers = []
        fc_input_size = attention_config['hidden_dim']
        
        for fc_config in config['model']['fully_connected']:
            fc_layers.extend([
                nn.Linear(fc_input_size, fc_config['units']),
                nn.ReLU(),
                nn.Dropout(fc_config['dropout'])
            ])
            fc_input_size = fc_config['units']
        
        # Output layer
        fc_layers.append(nn.Linear(fc_input_size, config['model']['num_classes']))
        self.classifier = nn.Sequential(*fc_layers)
        
        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
    
    def _get_cnn_output_size(self, input_length: int) -> int:
        # Create dummy input to calculate output size
        x = torch.randn(1, 1, input_length)
        with torch.no_grad():
            x = self.cnn(x)
        return x.size(-1)
    
    def forward(self, x):
        # Input shape: (batch_size, input_length)
        # Add channel dimension: (batch_size, 1, input_length)
        x = x.unsqueeze(1)
        
        # CNN feature extraction
        x = self.cnn(x)  # (batch_size, channels, reduced_length)
        
        # Prepare for attention: (batch_size, seq_len, features)
        x = x.transpose(1, 2)
        
        # Apply attention
        x = self.attention(x)
        
        # Global average pooling
        x = x.transpose(1, 2)  # Back to (batch_size, features, seq_len)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, features)
        
        # Classification
        output = self.classifier(x)
        
        return output

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
def load_model(model_file=None):
    config = DEFAULT_CONFIG
    model = SoundAnomalyDetector(config)
    
    if model_file is not None:
        try:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model_data = torch.load(io.BytesIO(model_file.read()), map_location='cpu')
            model.load_state_dict(model_data)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.success("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        # referenceãƒ•ã‚©ãƒ«ãƒ€ã®best_model.pthã‚’ãƒã‚§ãƒƒã‚¯
        reference_model_path = Path('reference/best_model.pth')
        default_model_path = Path('models/best_model.pth')
        
        model_loaded = False
        
        # ã¾ãšreferenceãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦ã™
        if reference_model_path.exists():
            try:
                state_dict = torch.load(reference_model_path, map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("reference ãƒ•ã‚©ãƒ«ãƒ€ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.success("âœ… referenceãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                model_loaded = True
            except Exception as e:
                logger.warning(f"referenceãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        # æ¬¡ã«modelsãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦ã™
        if not model_loaded and default_model_path.exists():
            try:
                state_dict = torch.load(default_model_path, map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("models ãƒ•ã‚©ãƒ«ãƒ€ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.info("ğŸ“ modelsãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ä¸­")
                model_loaded = True
            except Exception as e:
                logger.warning(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        if not model_loaded:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™")
            st.info("ğŸ¤– ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            _initialize_baseline_model(model)
    
    model.eval()
    return model

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def _initialize_baseline_model(model):
    try:
        for name, param in model.named_parameters():
            if 'weight' in name:
                if len(param.shape) > 1:
                    torch.nn.init.xavier_uniform_(param)
                else:
                    torch.nn.init.uniform_(param, -0.1, 0.1)
            elif 'bias' in name:
                torch.nn.init.constant_(param, 0)
        logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")

# éŸ³å£°å‰å‡¦ç†
def preprocess_audio(audio_data, sample_rate=44100):
    # æ­£è¦åŒ–
    audio_data = audio_data.astype(np.float32)
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    # 1ç§’é–“ï¼ˆ44100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«èª¿æ•´
    if len(audio_data) > sample_rate:
        audio_data = audio_data[:sample_rate]
    elif len(audio_data) < sample_rate:
        audio_data = np.pad(audio_data, (0, sample_rate - len(audio_data)), mode='constant')
    
    return audio_data

# éŸ³å£°åˆ†æ
def analyze_audio(audio_data, model, sample_rate=44100):
    # 1ç§’æ¯ã«åˆ†å‰²
    segments = []
    predictions = []
    confidences = []
    
    total_seconds = len(audio_data) // sample_rate
    
    for i in range(total_seconds):
        start_idx = i * sample_rate
        end_idx = start_idx + sample_rate
        segment = audio_data[start_idx:end_idx]
        
        # å‰å‡¦ç†
        processed_segment = preprocess_audio(segment, sample_rate)
        segments.append(processed_segment)
        
        # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        with torch.no_grad():
            input_tensor = torch.tensor(processed_segment).unsqueeze(0)
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            
            confidence, prediction = torch.max(probabilities, 1)
            predictions.append(prediction.item())
            confidences.append(confidence.item())
    
    return segments, predictions, confidences

# çµæœãƒ—ãƒ­ãƒƒãƒˆ
def plot_results(audio_data, predictions, sample_rate=44100):
    fig, ax = plt.subplots(figsize=(15, 6))
    
    # æ™‚é–“è»¸
    time_axis = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))
    
    # æ³¢å½¢ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(time_axis, audio_data, color='blue', alpha=0.7, linewidth=0.5)
    
    # OK/NGåŒºé–“ã®èƒŒæ™¯è‰²
    for i, pred in enumerate(predictions):
        start_time = i
        end_time = i + 1
        color = 'lightgreen' if pred == 0 else 'lightcoral'
        label = 'OK' if pred == 0 else 'NG'
        
        ax.axvspan(start_time, end_time, alpha=0.3, color=color)
        
        # ä¸­å¤®ã«ãƒ©ãƒ™ãƒ«è¡¨ç¤º
        mid_time = start_time + 0.5
        ax.text(mid_time, max(audio_data) * 0.8, label, 
                ha='center', va='center', fontsize=12, fontweight='bold')
    
    ax.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax.set_ylabel('æŒ¯å¹…', fontsize=12)
    ax.set_title('éŸ³å£°æ³¢å½¢ã¨ç•°å¸¸æ¤œçŸ¥çµæœ', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # å‡¡ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', alpha=0.5, label='OK (æ­£å¸¸)'),
        Patch(facecolor='lightcoral', alpha=0.5, label='NG (ç•°å¸¸)')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    return fig

# WebRTCéŸ³å£°éŒ²éŸ³ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
class AudioProcessor:
    def __init__(self):
        self.audio_frames = []
        self.recording = False
        
    def recv(self, frame):
        if self.recording:
            sound = frame.to_ndarray()
            self.audio_frames.append(sound)
        return frame

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    st.title("ğŸµ éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ—ãƒª")
    st.markdown("**ãƒã‚¤ã‚¯ã§éŒ²éŸ³ã—ã€1ç§’æ¯ã«OK/NGã‚’åˆ¤å®šã—ã¦æ³¢å½¢ä¸Šã«è¡¨ç¤ºã—ã¾ã™**")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.sidebar.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    uploaded_model = st.sidebar.file_uploader(
        "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (.pth) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['pth'],
        help="è¨“ç·´æ¸ˆã¿PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    sample_rate = st.sidebar.selectbox("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°", [22050, 44100], index=1)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if 'model' not in st.session_state or uploaded_model:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            st.session_state.model = load_model(uploaded_model)
    
    # WebRTCè¨­å®š
    RTC_CONFIGURATION = RTCConfiguration({
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    })
    
    # éŸ³å£°éŒ²éŸ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.header("ğŸ¤ éŸ³å£°éŒ²éŸ³")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("**ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŒ²éŸ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„**")
        
        # WebRTCéŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
        audio_processor = AudioProcessor()
        
        webrtc_ctx = webrtc_streamer(
            key="audio-recorder",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={
                "video": False,
                "audio": {
                    "sampleRate": sample_rate,
                    "channelCount": 1,
                    "echoCancellation": False,
                    "noiseSuppression": False,
                    "autoGainControl": False,
                }
            },
            audio_html_attrs={"autoPlay": True, "controls": False, "muted": False},
        )
        
        if webrtc_ctx.audio_receiver:
            # éŒ²éŸ³çŠ¶æ…‹ã®åˆæœŸåŒ–
            if 'recording' not in st.session_state:
                st.session_state.recording = False
            if 'audio_buffer' not in st.session_state:
                st.session_state.audio_buffer = []
            
            # éŒ²éŸ³åˆ¶å¾¡ãƒœã‚¿ãƒ³
            col_rec1, col_rec2 = st.columns(2)
            with col_rec1:
                if st.button("ğŸ™ï¸ éŒ²éŸ³é–‹å§‹", type="primary", disabled=st.session_state.recording):
                    st.session_state.recording = True
                    st.session_state.audio_buffer = []
                    st.success("éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼")
                    st.rerun()
                    
            with col_rec2:
                if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", type="secondary", disabled=not st.session_state.recording):
                    st.session_state.recording = False
                    st.info("éŒ²éŸ³ã‚’åœæ­¢ã—ã¾ã—ãŸ")
                    st.rerun()
            
            # éŒ²éŸ³çŠ¶æ…‹ã®è¡¨ç¤º
            if st.session_state.recording:
                st.warning("ğŸ”´ éŒ²éŸ³ä¸­... åœæ­¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")
            elif st.session_state.audio_buffer:
                try:
                    valid_chunks = [chunk for chunk in st.session_state.audio_buffer if len(chunk) > 0]
                    if valid_chunks:
                        buffer_data = np.concatenate(valid_chunks)
                        buffer_duration = len(buffer_data) / sample_rate
                        st.info(f"ğŸ“Š éŒ²éŸ³æ¸ˆã¿: {buffer_duration:.1f}ç§’ ({len(valid_chunks)} ãƒãƒ£ãƒ³ã‚¯)")
                    else:
                        st.info("ğŸ“Š éŒ²éŸ³æ¸ˆã¿: 0.0ç§’")
                except Exception as e:
                    logger.warning(f"Duration calculation error: {e}")
                    st.info("ğŸ“Š éŒ²éŸ³æ¸ˆã¿: è¨ˆç®—ä¸­...")
                
            # éŒ²éŸ³ä¸­ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            if webrtc_ctx.audio_receiver and st.session_state.get('recording', False):
                try:
                    # ã‚ˆã‚Šé•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§å®‰å®šã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
                    audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1.0)
                    if audio_frames:
                        for audio_frame in audio_frames:
                            sound = audio_frame.to_ndarray()
                            
                            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                            logger.debug(f"Audio frame shape: {sound.shape}, dtype: {sound.dtype}")
                            
                            # è¤‡æ•°ãƒãƒ£ãƒ³ãƒãƒ«ã®å ´åˆã¯ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
                            if len(sound.shape) > 1:
                                if sound.shape[1] > 1:  # ã‚¹ãƒ†ãƒ¬ã‚ªç­‰
                                    sound = np.mean(sound, axis=1)
                                else:  # æ—¢ã«ãƒ¢ãƒãƒ©ãƒ«ï¼ˆshape: [samples, 1]ï¼‰
                                    sound = sound.flatten()
                            
                            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’float32ã«å¤‰æ›
                            if sound.dtype != np.float32:
                                sound = sound.astype(np.float32)
                            
                            # æŒ¯å¹…ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ã¦ç•°å¸¸å€¤ã‚’é˜²ã
                            sound = np.clip(sound, -1.0, 1.0)
                            
                            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                            if len(sound) > 0:
                                st.session_state.audio_buffer.append(sound)
                                logger.debug(f"Audio chunk added: {len(sound)} samples")
                            
                except Exception as e:
                    error_msg = str(e).lower()
                    if "timeout" in error_msg:
                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯æ­£å¸¸ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒãªã„æ™‚ï¼‰
                        logger.debug("Audio frame timeout (normal)")
                    else:
                        logger.warning(f"éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        # é‡å¤§ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿éŒ²éŸ³ã‚’åœæ­¢
                        if "connection" in error_msg or "stream" in error_msg:
                            st.session_state.recording = False
                            st.error(f"éŒ²éŸ³æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
                            st.rerun()
        
        # éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        audio_buffer_available = bool(st.session_state.get('audio_buffer'))
        analysis_disabled = not audio_buffer_available or st.session_state.get('recording', False)
        
        if st.button("ğŸ” éŸ³å£°åˆ†æ", disabled=analysis_disabled):
            if st.session_state.get('audio_buffer'):
                try:
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæ”¹å–„ç‰ˆï¼‰
                    buffer_chunks = st.session_state.audio_buffer
                    valid_chunks = [chunk for chunk in buffer_chunks if len(chunk) > 0]
                    
                    if not valid_chunks:
                        st.error("æœ‰åŠ¹ãªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                        return
                    
                    audio_data = np.concatenate(valid_chunks)
                    logger.info(f"éŸ³å£°ãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(audio_data)} ã‚µãƒ³ãƒ—ãƒ« ({len(audio_data)/sample_rate:.1f}ç§’)")
                    
                    if len(audio_data) >= sample_rate:  # æœ€ä½1ç§’å¿…è¦
                        st.success(f"âœ… éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¾ã—ãŸï¼ ({len(audio_data)/sample_rate:.1f}ç§’)")
                        
                        # éŸ³å£°åˆ†æ
                        with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                            segments, predictions, confidences = analyze_audio(
                                audio_data, st.session_state.model, sample_rate
                            )
                        
                        # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                        st.session_state.audio_data = audio_data
                        st.session_state.predictions = predictions
                        st.session_state.confidences = confidences
                        st.session_state.sample_rate = sample_rate
                        st.success("ğŸ¯ éŸ³å£°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    else:
                        st.error(f"éŒ²éŸ³æ™‚é–“ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(audio_data)/sample_rate:.1f}ç§’ï¼‰ã€‚æœ€ä½1ç§’ä»¥ä¸ŠéŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
                        
                except Exception as e:
                    logger.error(f"éŸ³å£°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    st.error(f"éŸ³å£°åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                    if st.session_state.get('audio_buffer'):
                        buffer_info = [f"ãƒãƒ£ãƒ³ã‚¯{i}: {len(chunk)} ã‚µãƒ³ãƒ—ãƒ«" for i, chunk in enumerate(st.session_state.audio_buffer[:5])]
                        st.info(f"ãƒãƒƒãƒ•ã‚¡æƒ…å ±ï¼ˆæœ€åˆã®5ãƒãƒ£ãƒ³ã‚¯ï¼‰: {', '.join(buffer_info)}")
            else:
                st.error("éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
    
    with col2:
        st.markdown("### ğŸ“Š éŒ²éŸ³çŠ¶æ³")
        
        if st.session_state.get('audio_buffer'):
            try:
                # ãƒãƒƒãƒ•ã‚¡å†…ã®å„ãƒãƒ£ãƒ³ã‚¯ã®é•·ã•ã‚’ç¢ºèª
                buffer_chunks = st.session_state.audio_buffer
                if buffer_chunks:
                    # ç©ºã§ãªã„ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’çµåˆ
                    valid_chunks = [chunk for chunk in buffer_chunks if len(chunk) > 0]
                    if valid_chunks:
                        buffer_data = np.concatenate(valid_chunks)
                        duration = len(buffer_data) / sample_rate
                        st.metric("éŒ²éŸ³æ™‚é–“", f"{duration:.1f}ç§’")
                        st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", f"{len(buffer_data):,} ã‚µãƒ³ãƒ—ãƒ«")
                        st.metric("éŸ³å£°ãƒãƒ£ãƒ³ã‚¯", f"{len(valid_chunks)} å€‹")
                    else:
                        st.metric("éŒ²éŸ³æ™‚é–“", "0.0ç§’")
                        st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", "0 ã‚µãƒ³ãƒ—ãƒ«")
                else:
                    st.metric("éŒ²éŸ³æ™‚é–“", "0.0ç§’")
            except Exception as e:
                logger.warning(f"ãƒãƒƒãƒ•ã‚¡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                st.metric("éŒ²éŸ³æ™‚é–“", "è¨ˆç®—ã‚¨ãƒ©ãƒ¼")
        else:
            st.metric("éŒ²éŸ³æ™‚é–“", "0.0ç§’")
            
        if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢
            for key in ['audio_data', 'predictions', 'confidences', 'audio_buffer', 'recording']:
                if key in st.session_state:
                    del st.session_state[key]
            st.success("ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            st.rerun()
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ï¼ˆä»£æ›¿æ‰‹æ®µï¼‰
    st.header("ğŸ“ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»£æ›¿æ‰‹æ®µï¼‰")
    uploaded_audio = st.file_uploader(
        "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['wav', 'mp3', 'flac', 'm4a'],
        help="éŒ²éŸ³ã®ä»£ã‚ã‚Šã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™"
    )
    
    if uploaded_audio:
        with st.spinner("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."):
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{uploaded_audio.name.split(".")[-1]}') as tmp_file:
                    tmp_file.write(uploaded_audio.read())
                    tmp_path = tmp_file.name
                
                # librosã§éŸ³å£°èª­ã¿è¾¼ã¿
                audio_data, sr = librosa.load(tmp_path, sr=sample_rate, mono=True)
                
                if len(audio_data) > sample_rate:  # æœ€ä½1ç§’å¿…è¦
                    st.success("âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
                    
                    # éŸ³å£°åˆ†æ
                    with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                        segments, predictions, confidences = analyze_audio(
                            audio_data, st.session_state.model, sample_rate
                        )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.audio_data = audio_data
                    st.session_state.predictions = predictions
                    st.session_state.confidences = confidences
                    st.session_state.sample_rate = sample_rate
                else:
                    st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒçŸ­ã™ãã¾ã™ã€‚æœ€ä½1ç§’ä»¥ä¸Šã®éŸ³å£°ãŒå¿…è¦ã§ã™ã€‚")
                    
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                Path(tmp_path).unlink()
                
            except Exception as e:
                st.error(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœè¡¨ç¤º
    if 'audio_data' in st.session_state and 'predictions' in st.session_state:
        st.header("ğŸ“Š çµæœ")
        
        # çµ±è¨ˆæƒ…å ±
        predictions = st.session_state.predictions
        ok_count = predictions.count(0)
        ng_count = predictions.count(1)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç·æ™‚é–“", f"{len(predictions)}ç§’")
        with col2:
            st.metric("OKï¼ˆæ­£å¸¸ï¼‰", f"{ok_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", delta=None)
        with col3:
            st.metric("NGï¼ˆç•°å¸¸ï¼‰", f"{ng_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", delta=None)
        
        # æ³¢å½¢ã‚°ãƒ©ãƒ•
        fig = plot_results(st.session_state.audio_data, predictions, st.session_state.sample_rate)
        st.pyplot(fig)
        
        # è©³ç´°çµæœ
        with st.expander("ğŸ“‹ è©³ç´°çµæœ"):
            for i, (pred, conf) in enumerate(zip(predictions, st.session_state.confidences)):
                status = "âœ… OK" if pred == 0 else "âŒ NG"
                st.write(f"{i+1}ç§’ç›®: {status} (ä¿¡é ¼åº¦: {conf:.3f})")
    
    # èª¬æ˜
    st.markdown("---")
    st.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    #### ğŸ¤ éŸ³å£°éŒ²éŸ³æ–¹å¼:
    1. **ãƒã‚¤ã‚¯è¨±å¯**: ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ã‚¯ä½¿ç”¨è¨±å¯ã‚’ä¸ãˆã‚‹
    2. **éŒ²éŸ³é–‹å§‹**: ã€ŒğŸ™ï¸ éŒ²éŸ³é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™
    3. **éŒ²éŸ³åœæ­¢**: ã€Œâ¹ï¸ éŒ²éŸ³åœæ­¢ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™
    4. **åˆ†æå®Ÿè¡Œ**: ã€ŒğŸ” éŸ³å£°åˆ†æã€ãƒœã‚¿ãƒ³ã§è§£æé–‹å§‹
    
    #### ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹å¼:
    1. **ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ**: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAV, MP3ç­‰ï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    2. **è‡ªå‹•åˆ†æ**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«è‡ªå‹•ã§è§£æé–‹å§‹
    
    #### ğŸ¤– ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«:
    - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨“ç·´æ¸ˆã¿.pthãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½
    - ãƒ¢ãƒ‡ãƒ«ãªã—ã®å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    
    **åˆ¤å®šã«ã¤ã„ã¦:**
    - ğŸŸ¢ **OKï¼ˆæ­£å¸¸ï¼‰**: æ­£å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    - ğŸ”´ **NGï¼ˆç•°å¸¸ï¼‰**: ç•°å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    """)

if __name__ == "__main__":
    main()