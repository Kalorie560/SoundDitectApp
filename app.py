# è­¦å‘Šã‚’æœ€åˆã«æŠ‘åˆ¶ï¼ˆimportã®å‰ã«å®Ÿè¡Œï¼‰
import warnings
import os
warnings.filterwarnings("ignore")
# torch.classesè­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆStreamlitäº’æ›æ€§å•é¡Œï¼‰
warnings.filterwarnings("ignore", ".*torch._classes.*")
warnings.filterwarnings("ignore", ".*torch.*")
warnings.filterwarnings("ignore", ".*Examining the path of torch.classes.*")
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['PYTHONWARNINGS'] = 'ignore'
# PyTorch classesè­¦å‘Šã‚’å®Œå…¨ã«æŠ‘åˆ¶
os.environ['PYTHONHASHSEED'] = 'ignore'

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchaudio
import librosa
from pathlib import Path
import yaml
import logging
from typing import Tuple, Optional
import tempfile
import copy

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ—ãƒª - ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
    page_icon="ğŸµ",
    layout="wide"
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆreference/config.yamlã«åˆã‚ã›ãŸæ§‹é€ ï¼‰
DEFAULT_CONFIG = {
    'audio': {'sample_rate': 44100},
    'model': {
        'input_length': 44100, 
        'num_classes': 2,
        'cnn_layers': [
            {'filters': 64, 'kernel_size': 3, 'stride': 1, 'padding': 'same'},
            {'filters': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1},
            {'filters': 256, 'kernel_size': 3, 'stride': 2, 'padding': 1}
        ],
        'attention': {
            'hidden_dim': 256,
            'num_heads': 8
        },
        'fully_connected': [
            {'units': 512, 'dropout': 0.3},
            {'units': 256, 'dropout': 0.3}
        ]
    }
}

# Multi-head attention layer for audio feature processing
class AttentionLayer(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int):
        super(AttentionLayer, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        batch_size, seq_len, input_dim = x.size()
        
        # Compute queries, keys, values
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        attended = attended.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        output = self.output(attended)
        return output

# éŸ³å£°ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆ1D-CNN + Attentionæ©Ÿæ§‹ï¼‰
class SoundAnomalyDetector(nn.Module):
    def __init__(self, config: dict):
        super(SoundAnomalyDetector, self).__init__()
        self.config = config
        
        # CNN layers
        cnn_layers = []
        input_channels = 1
        
        for layer_config in config['model']['cnn_layers']:
            # Handle "same" padding by calculating appropriate padding
            padding = layer_config['padding']
            if padding == 'same':
                # For "same" padding in PyTorch, use kernel_size//2
                padding = layer_config['kernel_size'] // 2
            
            cnn_layers.extend([
                nn.Conv1d(
                    input_channels, 
                    layer_config['filters'],
                    kernel_size=layer_config['kernel_size'],
                    stride=layer_config['stride'],
                    padding=padding
                ),
                nn.BatchNorm1d(layer_config['filters']),
                nn.ReLU(),
                nn.MaxPool1d(2)
            ])
            input_channels = layer_config['filters']
        
        self.cnn = nn.Sequential(*cnn_layers)
        
        # Calculate CNN output size
        self.cnn_output_size = self._get_cnn_output_size(config['model']['input_length'])
        
        # Attention layer
        attention_config = config['model']['attention']
        self.attention = AttentionLayer(
            input_channels,
            attention_config['hidden_dim'],
            attention_config['num_heads']
        )
        
        # Fully connected layers
        fc_layers = []
        fc_input_size = attention_config['hidden_dim']
        
        for fc_config in config['model']['fully_connected']:
            fc_layers.extend([
                nn.Linear(fc_input_size, fc_config['units']),
                nn.ReLU(),
                nn.Dropout(fc_config['dropout'])
            ])
            fc_input_size = fc_config['units']
        
        # Output layer
        fc_layers.append(nn.Linear(fc_input_size, config['model']['num_classes']))
        self.classifier = nn.Sequential(*fc_layers)
        
        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
    
    def _get_cnn_output_size(self, input_length: int) -> int:
        # Create dummy input to calculate output size
        x = torch.randn(1, 1, input_length)
        with torch.no_grad():
            x = self.cnn(x)
        return x.size(-1)
    
    def forward(self, x):
        # Input shape: (batch_size, input_length)
        # Add channel dimension: (batch_size, 1, input_length)
        x = x.unsqueeze(1)
        
        # CNN feature extraction
        x = self.cnn(x)  # (batch_size, channels, reduced_length)
        
        # Prepare for attention: (batch_size, seq_len, features)
        x = x.transpose(1, 2)
        
        # Apply attention
        x = self.attention(x)
        
        # Global average pooling
        x = x.transpose(1, 2)  # Back to (batch_size, features, seq_len)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, features)
        
        # Classification
        output = self.classifier(x)
        
        return output

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
def load_config():
    """reference/config.yamlãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨"""
    config_path = Path('reference/config.yaml')
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
            logger.info("âœ… reference/config.yamlã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return yaml_config
        except Exception as e:
            logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        logger.info("reference/config.yamlãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
    
    return DEFAULT_CONFIG

# ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½
def detect_model_architecture(model_path: Path) -> Optional[dict]:
    """
    ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è‡ªå‹•çš„ã«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¤œå‡ºã™ã‚‹
    
    Args:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        æ¤œå‡ºã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãè¨­å®šè¾æ›¸ã€ã¾ãŸã¯ None
    """
    try:
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¤œæŸ»
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # CNNå±¤ã®æƒ…å ±ã‚’state_dictã‹ã‚‰æŠ½å‡º
        cnn_layers = []
        
        # é‡ã¿ã®å½¢çŠ¶ã‚’åˆ†æã—ã¦CNNå±¤ã‚’æ¤œå‡º
        layer_idx = 0
        while f'cnn.{layer_idx}.weight' in checkpoint:
            weight_shape = checkpoint[f'cnn.{layer_idx}.weight'].shape
            
            # CNNå±¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: é‡ã¿ã®å½¢çŠ¶ã¯ [out_channels, in_channels, kernel_size]
            if len(weight_shape) == 3:
                filters = weight_shape[0]
                in_channels = weight_shape[1]
                kernel_size = weight_shape[2]
                
                # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã¨ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨æ¸¬ï¼ˆå…¸å‹çš„ãªå€¤ï¼‰
                stride = 2 if layer_idx > 0 else 1  # æœ€åˆã®å±¤ã¯é€šå¸¸stride=1ã€ãã®ä»–ã¯stride=2
                padding = 1 if kernel_size == 3 else 0
                
                cnn_layers.append({
                    'filters': int(filters),
                    'kernel_size': int(kernel_size),
                    'stride': stride,
                    'padding': padding
                })
                
                logger.info(f"   æ¤œå‡ºã•ã‚ŒãŸCNNå±¤ {layer_idx//4}: {filters} ãƒ•ã‚£ãƒ«ã‚¿, ã‚«ãƒ¼ãƒãƒ« {kernel_size}")
            
            # æ¬¡ã®CNNå±¤ã«ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå„å±¤ã¯weight, bias, batch_norm weight, batch_norm biasã‚’æŒã¤ï¼‰
            layer_idx += 4
        
        if not cnn_layers:
            logger.warning("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰CNNå±¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã®æ¬¡å…ƒã‚’æ¤œå‡º
        attention_hidden_dim = 256  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        attention_num_heads = 8     # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã®é‡ã¿ã‹ã‚‰ hidden_dim ã‚’æ¤œå‡º
        if 'attention.query.weight' in checkpoint:
            attention_weight_shape = checkpoint['attention.query.weight'].shape
            attention_hidden_dim = attention_weight_shape[0]
            logger.info(f"   æ¤œå‡ºã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ hidden_dim: {attention_hidden_dim}")
        
        # å…¨çµåˆå±¤ã®æ¬¡å…ƒã‚’æ¤œå‡º
        fc_layers = []
        fc_idx = 0
        while f'classifier.{fc_idx}.weight' in checkpoint:
            weight_shape = checkpoint[f'classifier.{fc_idx}.weight'].shape
            
            if len(weight_shape) == 2:  # Linearå±¤
                out_features = weight_shape[0]
                in_features = weight_shape[1]
                
                # æœ€çµ‚å‡ºåŠ›å±¤ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆnum_classesï¼‰
                next_fc_idx = fc_idx + 3  # ReLUã¨Dropoutã‚’ã‚¹ã‚­ãƒƒãƒ—
                if f'classifier.{next_fc_idx}.weight' in checkpoint:
                    fc_layers.append({
                        'units': int(out_features),
                        'dropout': 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
                    })
                    logger.info(f"   æ¤œå‡ºã•ã‚ŒãŸFCå±¤ {len(fc_layers)}: {out_features} ãƒ¦ãƒ‹ãƒƒãƒˆ")
            
            fc_idx += 3  # ReLUã¨Dropoutå±¤ã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        # æ¤œå‡ºã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§è¨­å®šã‚’ä½œæˆ
        detected_config = copy.deepcopy(DEFAULT_CONFIG)  # å…ƒã®è¨­å®šã‚’ãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼
        
        # æ¤œå‡ºã•ã‚ŒãŸå€¤ã§æ›´æ–°
        detected_config['model']['cnn_layers'] = cnn_layers
        detected_config['model']['attention']['hidden_dim'] = attention_hidden_dim
        detected_config['model']['attention']['num_heads'] = attention_num_heads
        
        if fc_layers:
            detected_config['model']['fully_connected'] = fc_layers
        
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¤œå‡ºã«æˆåŠŸ:")
        logger.info(f"   CNNå±¤: {len(cnn_layers)}å±¤ã€ãƒ•ã‚£ãƒ«ã‚¿æ•° {[l['filters'] for l in cnn_layers]}")
        logger.info(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: {attention_hidden_dim}æ¬¡å…ƒã€{attention_num_heads}ãƒ˜ãƒƒãƒ‰")
        logger.info(f"   FCå±¤: {len(fc_layers)}å±¤")
        
        return detected_config
        
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
        return None

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
def load_model(model_file=None):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½ä»˜ãï¼‰
    """
    config = load_config()
    
    if model_file is not None:
        try:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            import io
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡ºã‚’å®Ÿè¡Œ
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pth') as tmp_file:
                tmp_file.write(model_file.read())
                tmp_path = tmp_file.name
            
            # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡ºã‚’è©¦è¡Œ
            detected_config = detect_model_architecture(Path(tmp_path))
            
            if detected_config:
                logger.info("ğŸ”§ æ¤œå‡ºã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­")
                model = SoundAnomalyDetector(detected_config)
                st.info("ğŸ” ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã—ãŸ")
            else:
                logger.info("ğŸ“‹ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­")
                model = SoundAnomalyDetector(config)
            
            # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿
            model_data = torch.load(tmp_path, map_location='cpu')
            model.load_state_dict(model_data)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            Path(tmp_path).unlink()
            
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.success("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            model = SoundAnomalyDetector(config)
            _initialize_baseline_model(model)
    else:
        # referenceãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        model_candidates = [
            Path('reference/best_model.pth'),
            Path('reference/old_best_model.pth'),
            Path('models/best_model.pth')
        ]
        
        model_loaded = False
        
        for model_path in model_candidates:
            if model_path.exists():
                try:
                    # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡ºã‚’è©¦è¡Œ
                    detected_config = detect_model_architecture(model_path)
                    
                    if detected_config:
                        logger.info(f"ğŸ”§ {model_path.name}ã‹ã‚‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¤œå‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­")
                        model = SoundAnomalyDetector(detected_config)
                        st.info(f"ğŸ” {model_path.name}ã‹ã‚‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã—ãŸ")
                    else:
                        logger.info(f"ğŸ“‹ {model_path.name}ã«å¯¾ã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­")
                        model = SoundAnomalyDetector(config)
                    
                    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿
                    state_dict = torch.load(model_path, map_location='cpu')
                    model.load_state_dict(state_dict)
                    
                    logger.info(f"{model_path}ã‹ã‚‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                    st.success(f"âœ… {model_path.name}ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                    model_loaded = True
                    break
                    
                except Exception as e:
                    logger.warning(f"{model_path}ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                    continue
        
        if not model_loaded:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™")
            st.info("ğŸ¤– ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            model = SoundAnomalyDetector(config)
            _initialize_baseline_model(model)
    
    model.eval()
    return model

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def _initialize_baseline_model(model):
    try:
        for name, param in model.named_parameters():
            if 'weight' in name:
                if len(param.shape) > 1:
                    torch.nn.init.xavier_uniform_(param)
                else:
                    torch.nn.init.uniform_(param, -0.1, 0.1)
            elif 'bias' in name:
                torch.nn.init.constant_(param, 0)
        logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")

# éŸ³å£°å‰å‡¦ç†
def preprocess_audio(audio_data, sample_rate=44100):
    # æ­£è¦åŒ–
    audio_data = audio_data.astype(np.float32)
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    # 1ç§’é–“ï¼ˆ44100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«èª¿æ•´
    if len(audio_data) > sample_rate:
        audio_data = audio_data[:sample_rate]
    elif len(audio_data) < sample_rate:
        audio_data = np.pad(audio_data, (0, sample_rate - len(audio_data)), mode='constant')
    
    return audio_data

# éŸ³å£°åˆ†æ
def analyze_audio(audio_data, model, sample_rate=44100):
    # 1ç§’æ¯ã«åˆ†å‰²
    segments = []
    predictions = []
    confidences = []
    
    total_seconds = len(audio_data) // sample_rate
    
    for i in range(total_seconds):
        start_idx = i * sample_rate
        end_idx = start_idx + sample_rate
        segment = audio_data[start_idx:end_idx]
        
        # å‰å‡¦ç†
        processed_segment = preprocess_audio(segment, sample_rate)
        segments.append(processed_segment)
        
        # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        with torch.no_grad():
            input_tensor = torch.tensor(processed_segment).unsqueeze(0)
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            
            confidence, prediction = torch.max(probabilities, 1)
            predictions.append(prediction.item())
            confidences.append(confidence.item())
    
    return segments, predictions, confidences

# çµæœãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
def plot_results(audio_data, predictions, sample_rate=44100, confidences=None):
    fig, ax = plt.subplots(figsize=(15, 6))
    
    # æ™‚é–“è»¸
    time_axis = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))
    
    # æ³¢å½¢ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(time_axis, audio_data, color='blue', alpha=0.7, linewidth=0.5)
    
    # OK/NGåŒºé–“ã®èƒŒæ™¯è‰²
    for i, pred in enumerate(predictions):
        start_time = i
        end_time = i + 1
        color = 'lightgreen' if pred == 0 else 'lightcoral'
        label = 'OK' if pred == 0 else 'NG'
        
        # ä¿¡é ¼åº¦ã«å¿œã˜ã¦ã‚¢ãƒ«ãƒ•ã‚¡å€¤ã‚’èª¿æ•´
        if confidences and i < len(confidences):
            alpha = 0.2 + (confidences[i] * 0.4)  # 0.2-0.6ã®ç¯„å›²
            confidence_text = f" ({confidences[i]:.2f})"
        else:
            alpha = 0.3
            confidence_text = ""
        
        ax.axvspan(start_time, end_time, alpha=alpha, color=color)
        
        # ä¸­å¤®ã«ãƒ©ãƒ™ãƒ«è¡¨ç¤ºï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
        mid_time = start_time + 0.5
        ax.text(mid_time, max(audio_data) * 0.8, f"{label}{confidence_text}", 
                ha='center', va='center', fontsize=10, fontweight='bold')
        
        # ä¿¡é ¼åº¦ãƒãƒ¼ã‚’ä¸‹éƒ¨ã«è¡¨ç¤º
        if confidences and i < len(confidences):
            bar_height = min(audio_data) * 0.1 * confidences[i]
            ax.axvspan(start_time, end_time, ymin=0, ymax=0.05, 
                      alpha=0.8, color='darkblue')
    
    ax.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax.set_ylabel('æŒ¯å¹…', fontsize=12)
    ax.set_title('éŸ³å£°æ³¢å½¢ã¨ç•°å¸¸æ¤œçŸ¥çµæœï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # å‡¡ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', alpha=0.5, label='OK (æ­£å¸¸)'),
        Patch(facecolor='lightcoral', alpha=0.5, label='NG (ç•°å¸¸)'),
        Patch(facecolor='darkblue', alpha=0.8, label='ä¿¡é ¼åº¦ãƒãƒ¼')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    return fig

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    st.title("ğŸµ éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ—ãƒª - ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.markdown("**éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€1ç§’æ¯ã«OK/NGã‚’åˆ¤å®šã—ã¾ã™**")
    
    # æ©Ÿèƒ½æ”¹è¨‚ã®èª¬æ˜
    st.info("""
    ğŸ”„ **ã‚¢ãƒ—ãƒªãŒæ–°ã—ããªã‚Šã¾ã—ãŸï¼**
    
    - ğŸ“¤ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å°‚ç”¨**: ã‚ˆã‚Šå®‰å®šã—ãŸéŸ³å£°åˆ†æä½“é¨“ã‚’æä¾›
    - ğŸ” **è‡ªå‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡º**: ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«å½¢çŠ¶ã«è‡ªå‹•å¯¾å¿œ
    - ğŸ¯ **ã‚·ãƒ³ãƒ—ãƒ«ã§ç›´æ„Ÿçš„**: è¤‡é›‘ãªéŒ²éŸ³è¨­å®šã¯ä¸è¦
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.sidebar.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    uploaded_model = st.sidebar.file_uploader(
        "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (.pth) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['pth'],
        help="è¨“ç·´æ¸ˆã¿PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    sample_rate = st.sidebar.selectbox("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°", [22050, 44100], index=1)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if 'model' not in st.session_state or uploaded_model:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            st.session_state.model = load_model(uploaded_model)
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    st.header("ğŸ“¤ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    uploaded_audio = st.file_uploader(
        "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", 
        type=['wav', 'mp3', 'flac', 'm4a', 'aac', 'ogg'],
        help="å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: WAV, MP3, FLAC, M4A, AAC, OGG"
    )
    
    if uploaded_audio:
        with st.spinner("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."):
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{uploaded_audio.name.split(".")[-1]}') as tmp_file:
                    tmp_file.write(uploaded_audio.read())
                    tmp_path = tmp_file.name
                
                # librosã§éŸ³å£°èª­ã¿è¾¼ã¿
                audio_data, sr = librosa.load(tmp_path, sr=sample_rate, mono=True)
                
                if len(audio_data) > sample_rate:  # æœ€ä½1ç§’å¿…è¦
                    st.success(f"âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ ({uploaded_audio.name})")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                    duration = len(audio_data) / sample_rate
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ãƒ•ã‚¡ã‚¤ãƒ«å", uploaded_audio.name)
                    with col2:
                        st.metric("å†ç”Ÿæ™‚é–“", f"{duration:.1f}ç§’")
                    with col3:
                        st.metric("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°", f"{sample_rate}Hz")
                    
                    # éŸ³å£°åˆ†æ
                    with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                        segments, predictions, confidences = analyze_audio(
                            audio_data, st.session_state.model, sample_rate
                        )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.audio_data = audio_data
                    st.session_state.predictions = predictions
                    st.session_state.confidences = confidences
                    st.session_state.sample_rate = sample_rate
                    
                    st.success("ğŸ¯ éŸ³å£°åˆ†æå®Œäº†ï¼")
                    
                else:
                    st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒçŸ­ã™ãã¾ã™ã€‚æœ€ä½1ç§’ä»¥ä¸Šã®éŸ³å£°ãŒå¿…è¦ã§ã™ã€‚")
                    
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                Path(tmp_path).unlink()
                
            except Exception as e:
                st.error(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                logger.error(f"Audio file processing error: {e}")
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã‚¬ã‚¤ãƒ‰
        st.markdown("""
        ### ğŸ“‹ ä½¿ç”¨æ–¹æ³•
        
        1. **ä¸Šè¨˜ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯**ã—ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        2. **ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**: WAV, MP3, FLAC, M4A, AAC, OGG
        3. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: æœ€å¤§200MBï¼ˆStreamlitã®åˆ¶é™ï¼‰
        4. **éŸ³å£°é•·**: æœ€ä½1ç§’ä»¥ä¸Š
        
        ### ğŸ’¡ å¯¾å¿œæ©Ÿèƒ½
        
        - ğŸ” **è‡ªå‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡º**: ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒã«è‡ªå‹•å¯¾å¿œ
        - ğŸ“Š **è©³ç´°åˆ†æçµæœ**: 1ç§’æ¯ã®åˆ¤å®šã¨ä¿¡é ¼åº¦è¡¨ç¤º
        - ğŸ“ˆ **è¦–è¦šçš„çµæœ**: æ³¢å½¢ã‚°ãƒ©ãƒ•ã¨è‰²åˆ†ã‘è¡¨ç¤º
        """)
    
    # çµæœè¡¨ç¤º
    if 'audio_data' in st.session_state and 'predictions' in st.session_state:
        st.header("ğŸ“Š åˆ†æçµæœ")
        
        # çµ±è¨ˆæƒ…å ±
        predictions = st.session_state.predictions
        confidences = st.session_state.confidences
        ok_count = predictions.count(0)
        ng_count = predictions.count(1)
        
        # ä¿¡é ¼åº¦çµ±è¨ˆ
        ok_confidences = [conf for pred, conf in zip(predictions, confidences) if pred == 0]
        ng_confidences = [conf for pred, conf in zip(predictions, confidences) if pred == 1]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·æ™‚é–“", f"{len(predictions)}ç§’")
        with col2:
            st.metric("OKï¼ˆæ­£å¸¸ï¼‰", f"{ok_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", 
                     delta=f"ä¿¡é ¼åº¦: {sum(ok_confidences)/len(ok_confidences):.2f}" if ok_confidences else "ä¿¡é ¼åº¦: N/A")
        with col3:
            st.metric("NGï¼ˆç•°å¸¸ï¼‰", f"{ng_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ",
                     delta=f"ä¿¡é ¼åº¦: {sum(ng_confidences)/len(ng_confidences):.2f}" if ng_confidences else "ä¿¡é ¼åº¦: N/A")
        with col4:
            st.metric("å¹³å‡ä¿¡é ¼åº¦", f"{avg_confidence:.3f}")
        
        # ä¿¡é ¼åº¦ã®è‰²åˆ†ã‘è¡¨ç¤º
        st.subheader("ğŸ¯ ä¿¡é ¼åº¦ã‚µãƒãƒªãƒ¼")
        
        confidence_cols = st.columns(len(predictions))
        for i, (pred, conf) in enumerate(zip(predictions, confidences)):
            with confidence_cols[i]:
                status = "âœ… OK" if pred == 0 else "âŒ NG"
                color = "green" if pred == 0 else "red"
                st.markdown(f"**{i+1}ç§’ç›®**")
                st.markdown(f"<div style='color: {color}; font-weight: bold;'>{status}</div>", unsafe_allow_html=True)
                st.progress(conf)
                st.caption(f"ä¿¡é ¼åº¦: {conf:.3f}")
        
        # æ³¢å½¢ã‚°ãƒ©ãƒ•ï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
        fig = plot_results(st.session_state.audio_data, predictions, 
                          st.session_state.sample_rate, confidences)
        st.pyplot(fig)
        
        # è©³ç´°çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("ğŸ“‹ è©³ç´°çµæœãƒ†ãƒ¼ãƒ–ãƒ«")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        import pandas as pd
        
        results_data = []
        for i, (pred, conf) in enumerate(zip(predictions, confidences)):
            status_emoji = "âœ…" if pred == 0 else "âŒ"
            status_text = "OK (æ­£å¸¸)" if pred == 0 else "NG (ç•°å¸¸)"
            confidence_level = "é«˜" if conf > 0.8 else "ä¸­" if conf > 0.5 else "ä½"
            
            results_data.append({
                "æ™‚åˆ»": f"{i+1}ç§’ç›®",
                "åˆ¤å®š": f"{status_emoji} {status_text}",
                "ä¿¡é ¼åº¦": f"{conf:.3f}",
                "ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«": confidence_level,
                "ç¢ºä¿¡åº¦": f"{conf*100:.1f}%"
            })
        
        results_df = pd.DataFrame(results_data)
        st.dataframe(results_df, use_container_width=True)
    
    # èª¬æ˜
    st.markdown("---")
    st.markdown("### ğŸ“– ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦")
    st.markdown("""
    #### ğŸ¯ ä¸»ãªæ©Ÿèƒ½:
    - **ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: æ§˜ã€…ãªéŸ³å£°å½¢å¼ã«å¯¾å¿œ
    - **ğŸ” è‡ªå‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡º**: ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒã«è‡ªå‹•é©å¿œ
    - **ğŸ“Š è©³ç´°åˆ†æ**: 1ç§’æ¯ã®åˆ¤å®šã¨ä¿¡é ¼åº¦è¡¨ç¤º
    - **ğŸ“ˆ è¦–è¦šåŒ–**: æ³¢å½¢ã‚°ãƒ©ãƒ•ã¨çµæœã®è‰²åˆ†ã‘è¡¨ç¤º
    
    #### ğŸ¤– ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ:
    - **reference/best_model.pth**: æœ€æ–°ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    - **reference/old_best_model.pth**: æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã«ã‚‚å¯¾å¿œ
    - **ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«**: ç‹¬è‡ªã®.pthãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½
    
    #### ğŸ“ åˆ¤å®šã«ã¤ã„ã¦:
    - ğŸŸ¢ **OKï¼ˆæ­£å¸¸ï¼‰**: æ­£å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    - ğŸ”´ **NGï¼ˆç•°å¸¸ï¼‰**: ç•°å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    - ğŸ“Š **ä¿¡é ¼åº¦**: 0.000-1.000ã®ç¯„å›²ã§åˆ¤å®šã®ç¢ºä¿¡åº¦ã‚’è¡¨ç¤º
    """)

if __name__ == "__main__":
    main()