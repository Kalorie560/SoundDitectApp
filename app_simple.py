# è­¦å‘Šã‚’æœ€åˆã«æŠ‘åˆ¶ï¼ˆimportã®å‰ã«å®Ÿè¡Œï¼‰
import warnings
import os
warnings.filterwarnings("ignore")
# torch.classesè­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆStreamlitäº’æ›æ€§å•é¡Œï¼‰
warnings.filterwarnings("ignore", ".*torch._classes.*")
warnings.filterwarnings("ignore", ".*torch.*")
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['PYTHONWARNINGS'] = 'ignore'

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchaudio
import librosa
from pathlib import Path
import yaml
import time
import logging
from typing import Tuple, Optional
import tempfile
import threading
from datetime import datetime

# Simple recorderãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from simple_recorder import SimpleAudioRecorder

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="éŸ³å£°æ¤œå‡ºã‚¢ãƒ—ãƒª - Simple Recording",
    page_icon="ğŸµ",
    layout="wide"
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆreference/config.yamlã«åˆã‚ã›ãŸæ§‹é€ ï¼‰
DEFAULT_CONFIG = {
    'audio': {'sample_rate': 44100},
    'model': {
        'input_length': 44100, 
        'num_classes': 2,
        'cnn_layers': [
            {'filters': 64, 'kernel_size': 3, 'stride': 1, 'padding': 'same'},
            {'filters': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1},
            {'filters': 256, 'kernel_size': 3, 'stride': 2, 'padding': 1}
        ],
        'attention': {
            'hidden_dim': 256,
            'num_heads': 8
        },
        'fully_connected': [
            {'units': 512, 'dropout': 0.3},
            {'units': 256, 'dropout': 0.3}
        ]
    }
}

# Multi-head attention layer for audio feature processing
class AttentionLayer(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int):
        super(AttentionLayer, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        batch_size, seq_len, input_dim = x.size()
        
        # Compute queries, keys, values
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        attended = attended.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        output = self.output(attended)
        return output

# éŸ³å£°ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆ1D-CNN + Attentionæ©Ÿæ§‹ï¼‰
class SoundAnomalyDetector(nn.Module):
    def __init__(self, config: dict):
        super(SoundAnomalyDetector, self).__init__()
        self.config = config
        
        # CNN layers
        cnn_layers = []
        input_channels = 1
        
        for layer_config in config['model']['cnn_layers']:
            # Handle "same" padding by calculating appropriate padding
            padding = layer_config['padding']
            if padding == 'same':
                # For "same" padding in PyTorch, use kernel_size//2
                padding = layer_config['kernel_size'] // 2
            
            cnn_layers.extend([
                nn.Conv1d(
                    input_channels, 
                    layer_config['filters'],
                    kernel_size=layer_config['kernel_size'],
                    stride=layer_config['stride'],
                    padding=padding
                ),
                nn.BatchNorm1d(layer_config['filters']),
                nn.ReLU(),
                nn.MaxPool1d(2)
            ])
            input_channels = layer_config['filters']
        
        self.cnn = nn.Sequential(*cnn_layers)
        
        # Calculate CNN output size
        self.cnn_output_size = self._get_cnn_output_size(config['model']['input_length'])
        
        # Attention layer
        attention_config = config['model']['attention']
        self.attention = AttentionLayer(
            input_channels,
            attention_config['hidden_dim'],
            attention_config['num_heads']
        )
        
        # Fully connected layers
        fc_layers = []
        fc_input_size = attention_config['hidden_dim']
        
        for fc_config in config['model']['fully_connected']:
            fc_layers.extend([
                nn.Linear(fc_input_size, fc_config['units']),
                nn.ReLU(),
                nn.Dropout(fc_config['dropout'])
            ])
            fc_input_size = fc_config['units']
        
        # Output layer
        fc_layers.append(nn.Linear(fc_input_size, config['model']['num_classes']))
        self.classifier = nn.Sequential(*fc_layers)
        
        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
    
    def _get_cnn_output_size(self, input_length: int) -> int:
        # Create dummy input to calculate output size
        x = torch.randn(1, 1, input_length)
        with torch.no_grad():
            x = self.cnn(x)
        return x.size(-1)
    
    def forward(self, x):
        # Input shape: (batch_size, input_length)
        # Add channel dimension: (batch_size, 1, input_length)
        x = x.unsqueeze(1)
        
        # CNN feature extraction
        x = self.cnn(x)  # (batch_size, channels, reduced_length)
        
        # Prepare for attention: (batch_size, seq_len, features)
        x = x.transpose(1, 2)
        
        # Apply attention
        x = self.attention(x)
        
        # Global average pooling
        x = x.transpose(1, 2)  # Back to (batch_size, features, seq_len)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, features)
        
        # Classification
        output = self.classifier(x)
        
        return output

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
def load_config():
    """reference/config.yamlãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨"""
    config_path = Path('reference/config.yaml')
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
            logger.info("âœ… reference/config.yamlã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return yaml_config
        except Exception as e:
            logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        logger.info("reference/config.yamlãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
    
    return DEFAULT_CONFIG

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
def load_model(model_file=None):
    # è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆreference/config.yamlãŒã‚ã‚Œã°å„ªå…ˆä½¿ç”¨ï¼‰
    config = load_config()
    model = SoundAnomalyDetector(config)
    
    if model_file is not None:
        try:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            import io
            model_data = torch.load(io.BytesIO(model_file.read()), map_location='cpu')
            model.load_state_dict(model_data)
            logger.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.success("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        # referenceãƒ•ã‚©ãƒ«ãƒ€ã®best_model.pthã‚’ãƒã‚§ãƒƒã‚¯
        reference_model_path = Path('reference/best_model.pth')
        default_model_path = Path('models/best_model.pth')
        
        model_loaded = False
        
        # ã¾ãšreferenceãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦ã™
        if reference_model_path.exists():
            try:
                state_dict = torch.load(reference_model_path, map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("reference ãƒ•ã‚©ãƒ«ãƒ€ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.success("âœ… referenceãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                model_loaded = True
            except Exception as e:
                logger.warning(f"referenceãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        # æ¬¡ã«modelsãƒ•ã‚©ãƒ«ãƒ€ã‚’è©¦ã™
        if not model_loaded and default_model_path.exists():
            try:
                state_dict = torch.load(default_model_path, map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("models ãƒ•ã‚©ãƒ«ãƒ€ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.info("ğŸ“ modelsãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ä¸­")
                model_loaded = True
            except Exception as e:
                logger.warning(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        if not model_loaded:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™")
            st.info("ğŸ¤– ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            _initialize_baseline_model(model)
    
    model.eval()
    return model

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def _initialize_baseline_model(model):
    try:
        for name, param in model.named_parameters():
            if 'weight' in name:
                if len(param.shape) > 1:
                    torch.nn.init.xavier_uniform_(param)
                else:
                    torch.nn.init.uniform_(param, -0.1, 0.1)
            elif 'bias' in name:
                torch.nn.init.constant_(param, 0)
        logger.info("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")

# éŸ³å£°å‰å‡¦ç†
def preprocess_audio(audio_data, sample_rate=44100):
    # æ­£è¦åŒ–
    audio_data = audio_data.astype(np.float32)
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    # 1ç§’é–“ï¼ˆ44100ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«èª¿æ•´
    if len(audio_data) > sample_rate:
        audio_data = audio_data[:sample_rate]
    elif len(audio_data) < sample_rate:
        audio_data = np.pad(audio_data, (0, sample_rate - len(audio_data)), mode='constant')
    
    return audio_data

# éŸ³å£°åˆ†æ
def analyze_audio(audio_data, model, sample_rate=44100):
    # 1ç§’æ¯ã«åˆ†å‰²
    segments = []
    predictions = []
    confidences = []
    
    total_seconds = len(audio_data) // sample_rate
    
    for i in range(total_seconds):
        start_idx = i * sample_rate
        end_idx = start_idx + sample_rate
        segment = audio_data[start_idx:end_idx]
        
        # å‰å‡¦ç†
        processed_segment = preprocess_audio(segment, sample_rate)
        segments.append(processed_segment)
        
        # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        with torch.no_grad():
            input_tensor = torch.tensor(processed_segment).unsqueeze(0)
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            
            confidence, prediction = torch.max(probabilities, 1)
            predictions.append(prediction.item())
            confidences.append(confidence.item())
    
    return segments, predictions, confidences

# çµæœãƒ—ãƒ­ãƒƒãƒˆ
def plot_results(audio_data, predictions, sample_rate=44100):
    fig, ax = plt.subplots(figsize=(15, 6))
    
    # æ™‚é–“è»¸
    time_axis = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))
    
    # æ³¢å½¢ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(time_axis, audio_data, color='blue', alpha=0.7, linewidth=0.5)
    
    # OK/NGåŒºé–“ã®èƒŒæ™¯è‰²
    for i, pred in enumerate(predictions):
        start_time = i
        end_time = i + 1
        color = 'lightgreen' if pred == 0 else 'lightcoral'
        label = 'OK' if pred == 0 else 'NG'
        
        ax.axvspan(start_time, end_time, alpha=0.3, color=color)
        
        # ä¸­å¤®ã«ãƒ©ãƒ™ãƒ«è¡¨ç¤º
        mid_time = start_time + 0.5
        ax.text(mid_time, max(audio_data) * 0.8, label, 
                ha='center', va='center', fontsize=12, fontweight='bold')
    
    ax.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12)
    ax.set_ylabel('æŒ¯å¹…', fontsize=12)
    ax.set_title('éŸ³å£°æ³¢å½¢ã¨ç•°å¸¸æ¤œçŸ¥çµæœ', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # å‡¡ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', alpha=0.5, label='OK (æ­£å¸¸)'),
        Patch(facecolor='lightcoral', alpha=0.5, label='NG (ç•°å¸¸)')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    return fig

# WAVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢æ•°
def process_wav_file(file_path: str, model, sample_rate: int = 44100):
    """
    WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†
    
    Args:
        file_path: WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        model: éŸ³å£°æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°
        
    Returns:
        tuple: (éŸ³å£°ãƒ‡ãƒ¼ã‚¿, äºˆæ¸¬çµæœ, ä¿¡é ¼åº¦)
    """
    try:
        # WAVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        audio_data, sr = SimpleAudioRecorder.load_wav_file(file_path)
        
        if len(audio_data) == 0:
            return None, None, None
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if sr != sample_rate:
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=sample_rate)
        
        # éŸ³å£°åˆ†æ
        segments, predictions, confidences = analyze_audio(audio_data, model, sample_rate)
        
        return audio_data, predictions, confidences
        
    except Exception as e:
        logger.error(f"WAVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None, None

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    st.title("ğŸµ éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ—ãƒª - Simple Recording")
    st.markdown("**ã‚·ãƒ³ãƒ—ãƒ«ãªéŒ²éŸ³æ©Ÿèƒ½ã§WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€1ç§’æ¯ã«OK/NGã‚’åˆ¤å®šã—ã¾ã™**")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("âš™ï¸ è¨­å®š")
    
    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹æƒ…å ±è¡¨ç¤º
    with st.sidebar.expander("ğŸ¤ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹æƒ…å ±"):
        try:
            devices = SimpleAudioRecorder.get_available_devices()
            if devices:
                st.write("åˆ©ç”¨å¯èƒ½ãªå…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹:")
                for device in devices:
                    st.write(f"â€¢ {device['name']} ({device['channels']}ch)")
            else:
                st.warning("å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except Exception as e:
            st.error(f"ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.sidebar.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    uploaded_model = st.sidebar.file_uploader(
        "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (.pth) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['pth'],
        help="è¨“ç·´æ¸ˆã¿PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    sample_rate = st.sidebar.selectbox("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°", [22050, 44100], index=1)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if 'model' not in st.session_state or uploaded_model:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            st.session_state.model = load_model(uploaded_model)
    
    # éŒ²éŸ³è¨­å®š
    st.sidebar.subheader("ğŸ“¹ éŒ²éŸ³è¨­å®š")
    recording_duration = st.sidebar.slider("éŒ²éŸ³æ™‚é–“ (ç§’)", 1, 30, 5)
    
    # éŸ³å£°éŒ²éŸ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.header("ğŸ¤ éŸ³å£°éŒ²éŸ³ (WAVä¿å­˜)")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("**Simple Recorderã‚’ä½¿ç”¨ã—ãŸWAVéŒ²éŸ³æ©Ÿèƒ½**")
        
        # éŒ²éŸ³åˆ¶å¾¡
        if 'recorder' not in st.session_state:
            st.session_state.recorder = SimpleAudioRecorder(sample_rate=sample_rate)
        
        recorder = st.session_state.recorder
        
        # éŒ²éŸ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if 'is_recording' not in st.session_state:
            st.session_state.is_recording = False
        if 'last_recording_data' not in st.session_state:
            st.session_state.last_recording_data = None
        if 'last_wav_file' not in st.session_state:
            st.session_state.last_wav_file = None
        
        # éŒ²éŸ³ãƒœã‚¿ãƒ³
        col_rec1, col_rec2, col_rec3 = st.columns(3)
        
        with col_rec1:
            if st.button("ğŸ™ï¸ éŒ²éŸ³é–‹å§‹", type="primary", disabled=st.session_state.is_recording):
                try:
                    # éŒ²éŸ³ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    wav_filename = f"recording_{timestamp}.wav"
                    wav_path = Path("recordings") / wav_filename
                    
                    # recordingsãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
                    Path("recordings").mkdir(exist_ok=True)
                    
                    st.session_state.is_recording = True
                    st.session_state.current_wav_file = str(wav_path)
                    
                    # é€²æ—è¡¨ç¤ºç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # éŒ²éŸ³å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰
                    def record_audio():
                        def progress_callback(current_time, total_time):
                            progress = min(current_time / total_time, 1.0)
                            progress_bar.progress(progress)
                            status_text.text(f"éŒ²éŸ³ä¸­: {current_time:.1f}/{total_time:.1f}ç§’")
                        
                        success = recorder.record_and_save(
                            duration=recording_duration,
                            file_path=str(wav_path),
                            progress_callback=progress_callback
                        )
                        
                        st.session_state.is_recording = False
                        
                        if success:
                            st.session_state.last_wav_file = str(wav_path)
                            st.success(f"âœ… éŒ²éŸ³å®Œäº†ï¼ {wav_filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
                            
                            # éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                            audio_data, sr = SimpleAudioRecorder.load_wav_file(str(wav_path))
                            st.session_state.last_recording_data = audio_data
                            
                        else:
                            st.error("âŒ éŒ²éŸ³ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        
                        progress_bar.empty()
                        status_text.empty()
                    
                    # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
                    recording_thread = threading.Thread(target=record_audio)
                    recording_thread.start()
                    
                except Exception as e:
                    st.error(f"éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
                    st.session_state.is_recording = False
        
        with col_rec2:
            if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", type="secondary", disabled=not st.session_state.is_recording):
                if recorder.is_recording:
                    audio_data = recorder.stop_recording()
                    if len(audio_data) > 0:
                        # WAVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        wav_filename = f"recording_{timestamp}_manual.wav"
                        wav_path = Path("recordings") / wav_filename
                        Path("recordings").mkdir(exist_ok=True)
                        
                        if recorder.save_to_wav(audio_data, str(wav_path)):
                            st.session_state.last_wav_file = str(wav_path)
                            st.session_state.last_recording_data = audio_data
                            st.success(f"âœ… éŒ²éŸ³åœæ­¢ãƒ»ä¿å­˜å®Œäº†ï¼ {wav_filename}")
                        else:
                            st.error("âŒ WAVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    else:
                        st.warning("éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                
                st.session_state.is_recording = False
        
        with col_rec3:
            if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ"):
                # éŒ²éŸ³åœæ­¢
                if recorder.is_recording:
                    recorder.stop_recording()
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢
                for key in ['last_recording_data', 'last_wav_file', 'is_recording', 
                           'audio_data', 'predictions', 'confidences']:
                    if key in st.session_state:
                        del st.session_state[key]
                
                st.success("ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
                st.rerun()
        
        # éŒ²éŸ³çŠ¶æ…‹è¡¨ç¤º
        if st.session_state.is_recording:
            st.warning("ğŸ”´ éŒ²éŸ³ä¸­...")
        elif st.session_state.last_wav_file:
            st.info(f"ğŸ“ æœ€æ–°ã®éŒ²éŸ³: {Path(st.session_state.last_wav_file).name}")
        
        # WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æãƒœã‚¿ãƒ³
        if st.session_state.last_wav_file and not st.session_state.is_recording:
            if st.button("ğŸ” WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ", type="primary"):
                with st.spinner("WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­..."):
                    audio_data, predictions, confidences = process_wav_file(
                        st.session_state.last_wav_file, 
                        st.session_state.model, 
                        sample_rate
                    )
                    
                    if audio_data is not None:
                        # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                        st.session_state.audio_data = audio_data
                        st.session_state.predictions = predictions
                        st.session_state.confidences = confidences
                        st.session_state.sample_rate = sample_rate
                        st.success("ğŸ¯ WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Œäº†ï¼")
                    else:
                        st.error("WAVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    with col2:
        st.markdown("### ğŸ“Š éŒ²éŸ³çŠ¶æ³")
        
        if st.session_state.last_recording_data is not None:
            duration = len(st.session_state.last_recording_data) / sample_rate
            st.metric("éŒ²éŸ³æ™‚é–“", f"{duration:.1f}ç§’")
            st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", f"{len(st.session_state.last_recording_data):,} ã‚µãƒ³ãƒ—ãƒ«")
            st.metric("ãƒ•ã‚¡ã‚¤ãƒ«", Path(st.session_state.last_wav_file).name if st.session_state.last_wav_file else "ãªã—")
        else:
            st.metric("éŒ²éŸ³æ™‚é–“", "0.0ç§’")
            st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", "0 ã‚µãƒ³ãƒ—ãƒ«")
            st.metric("ãƒ•ã‚¡ã‚¤ãƒ«", "ãªã—")
    
    # ä¿å­˜ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    st.header("ğŸ“ ä¿å­˜ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«")
    recordings_path = Path("recordings")
    if recordings_path.exists():
        wav_files = list(recordings_path.glob("*.wav"))
        if wav_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
            selected_file = st.selectbox(
                "WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                options=[f.name for f in sorted(wav_files, reverse=True)],
                help="åˆ†æã—ãŸã„WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )
            
            if selected_file:
                selected_path = recordings_path / selected_file
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("ğŸ” é¸æŠãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"):
                        with st.spinner(f"{selected_file} ã‚’åˆ†æä¸­..."):
                            audio_data, predictions, confidences = process_wav_file(
                                str(selected_path), 
                                st.session_state.model, 
                                sample_rate
                            )
                            
                            if audio_data is not None:
                                st.session_state.audio_data = audio_data
                                st.session_state.predictions = predictions
                                st.session_state.confidences = confidences
                                st.session_state.sample_rate = sample_rate
                                st.success(f"âœ… {selected_file} ã®åˆ†æå®Œäº†ï¼")
                            else:
                                st.error(f"âŒ {selected_file} ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                with col2:
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    if selected_path.exists():
                        with open(selected_path, "rb") as f:
                            st.download_button(
                                label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=f.read(),
                                file_name=selected_file,
                                mime="audio/wav"
                            )
                
                with col3:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    if st.button("ğŸ—‘ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤", type="secondary"):
                        if selected_path.exists():
                            selected_path.unlink()
                            st.success(f"âœ… {selected_file} ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                            st.rerun()
        else:
            st.info("ä¿å­˜ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
    else:
        st.info("recordingsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    st.header("ğŸ“¤ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_audio = st.file_uploader(
        "å¤–éƒ¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", 
        type=['wav', 'mp3', 'flac', 'm4a'],
        help="å¤–éƒ¨ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã§ãã¾ã™"
    )
    
    if uploaded_audio:
        with st.spinner("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."):
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{uploaded_audio.name.split(".")[-1]}') as tmp_file:
                    tmp_file.write(uploaded_audio.read())
                    tmp_path = tmp_file.name
                
                # librosã§éŸ³å£°èª­ã¿è¾¼ã¿
                audio_data, sr = librosa.load(tmp_path, sr=sample_rate, mono=True)
                
                if len(audio_data) > sample_rate:  # æœ€ä½1ç§’å¿…è¦
                    st.success("âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
                    
                    # éŸ³å£°åˆ†æ
                    with st.spinner("éŸ³å£°ã‚’åˆ†æä¸­..."):
                        segments, predictions, confidences = analyze_audio(
                            audio_data, st.session_state.model, sample_rate
                        )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state.audio_data = audio_data
                    st.session_state.predictions = predictions
                    st.session_state.confidences = confidences
                    st.session_state.sample_rate = sample_rate
                else:
                    st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒçŸ­ã™ãã¾ã™ã€‚æœ€ä½1ç§’ä»¥ä¸Šã®éŸ³å£°ãŒå¿…è¦ã§ã™ã€‚")
                    
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                Path(tmp_path).unlink()
                
            except Exception as e:
                st.error(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœè¡¨ç¤º
    if 'audio_data' in st.session_state and 'predictions' in st.session_state:
        st.header("ğŸ“Š åˆ†æçµæœ")
        
        # çµ±è¨ˆæƒ…å ±
        predictions = st.session_state.predictions
        ok_count = predictions.count(0)
        ng_count = predictions.count(1)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç·æ™‚é–“", f"{len(predictions)}ç§’")
        with col2:
            st.metric("OKï¼ˆæ­£å¸¸ï¼‰", f"{ok_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
        with col3:
            st.metric("NGï¼ˆç•°å¸¸ï¼‰", f"{ng_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
        
        # æ³¢å½¢ã‚°ãƒ©ãƒ•
        fig = plot_results(st.session_state.audio_data, predictions, st.session_state.sample_rate)
        st.pyplot(fig)
        
        # è©³ç´°çµæœ
        with st.expander("ğŸ“‹ è©³ç´°çµæœ"):
            for i, (pred, conf) in enumerate(zip(predictions, st.session_state.confidences)):
                status = "âœ… OK" if pred == 0 else "âŒ NG"
                st.write(f"{i+1}ç§’ç›®: {status} (ä¿¡é ¼åº¦: {conf:.3f})")
    
    # èª¬æ˜
    st.markdown("---")
    st.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    #### ğŸ¤ Simple Recordingæ–¹å¼:
    1. **éŒ²éŸ³æ™‚é–“è¨­å®š**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§éŒ²éŸ³æ™‚é–“ã‚’é¸æŠ
    2. **éŒ²éŸ³é–‹å§‹**: ã€ŒğŸ™ï¸ éŒ²éŸ³é–‹å§‹ã€ãƒœã‚¿ãƒ³ã§WAVéŒ²éŸ³é–‹å§‹
    3. **è‡ªå‹•åœæ­¢**: è¨­å®šæ™‚é–“ã§è‡ªå‹•åœæ­¢ãƒ»WAVä¿å­˜
    4. **åˆ†æå®Ÿè¡Œ**: ã€ŒğŸ” WAVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã€ã§è§£æé–‹å§‹
    
    #### ğŸ“ WAVãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†:
    - éŒ²éŸ³ã•ã‚ŒãŸWAVãƒ•ã‚¡ã‚¤ãƒ«ã¯`recordings/`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
    - ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‹ã‚‰éå»ã®éŒ²éŸ³ã‚’é¸æŠãƒ»åˆ†æå¯èƒ½
    - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰Šé™¤æ©Ÿèƒ½ä»˜ã
    
    #### ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:
    - å¤–éƒ¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAV, MP3ç­‰ï¼‰ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»åˆ†æ
    
    **åˆ¤å®šã«ã¤ã„ã¦:**
    - ğŸŸ¢ **OKï¼ˆæ­£å¸¸ï¼‰**: æ­£å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    - ğŸ”´ **NGï¼ˆç•°å¸¸ï¼‰**: ç•°å¸¸ãªéŸ³å£°ã¨åˆ¤å®š
    
    **æ”¹å–„ç‚¹:**
    - WebRTCã«ã‚ˆã‚‹è¤‡é›‘ãªéŒ²éŸ³å‡¦ç†ã‚’æ’é™¤
    - sounddeviceã«ã‚ˆã‚‹å®‰å®šã—ãŸéŒ²éŸ³
    - WAVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ç›´æ¥ä¿å­˜
    - ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†æ©Ÿèƒ½ã®è¿½åŠ 
    """)

if __name__ == "__main__":
    main()